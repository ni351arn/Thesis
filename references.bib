@article{amteCloudNativeAIChallenges2025,
  title = {Cloud-{{Native AI}}: {{Challenges}} and {{Innovations}} in {{Deploying Large-Scale Machine Learning Models}}},
  shorttitle = {Cloud-{{Native AI}}},
  author = {Amte, Rahul},
  year = {2025},
  month = mar,
  journal = {ISCSITR - INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING (ISCSITR-IJSRAIML) ISSN (Online): 3067-753X},
  volume = {6},
  number = {2},
  pages = {9--18},
  urldate = {2025-11-05},
  abstract = {Large scale machine learning models are deployed with revolutionary speed in cloud native AI for the reasons of scalability, flexibility and cost efficiency. However, most of these challenges face a latency problem, resource management issue, and most of all security risk. This paper considers challenges in producing cloud native AI, such as containerization, microservices, and optimized inference pipelines and then it studies innovations in these areas. By qualitative and quantitative analysis, we examine best practices of deploying AI models effectively in cloud environment. The results indicate ways of cost effective and high-performance AI implementation, with a focus on automation, edge computation, and serverless hands in increasing the deployment of AI on a sustainable basis.},
  copyright = {Copyright (c) -1 ISCSITR - INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN  ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING (ISCSITR-IJSRAIML)},
  langid = {english},
  keywords = {AI,Cloud,Deployment,Machine Learning},
  file = {/Users/nilsarnold/Zotero/storage/63CZ2GJP/Amte - 2025 - Cloud-Native AI Challenges and Innovations in Deploying Large-Scale Machine Learning Models.pdf}
}

@article{bajwaCLOUDNATIVEARCHITECTURESLARGESCALE2025,
  title = {{{CLOUD-NATIVE ARCHITECTURES FOR LARGE-SCALE AI-BASED PREDICTIVE MODELING}}},
  author = {Bajwa, Muhammad Talha Tahir and Wattoo, Saman and Mehmood, Irum and Talha, Muhammad and Anwar, Muhammad Junaid and Ullah, Muhammad Sana},
  year = {2025},
  month = aug,
  journal = {Journal of Emerging Technology and Digital Transformation},
  volume = {4},
  number = {2},
  pages = {207--221},
  issn = {3006-9726},
  urldate = {2025-10-25},
  abstract = {The demand of adapted, expandable, efficient deployment techniques has become more acknowledged because of the accelerated growth of artificial intelligence (AI) initiatives and high intricacity of big forms of predictive modeling. Cloud-native architectures which are founded on concepts such as serverless computing, microservices, orchestration and containerization create a solid foundation in satisfying these needs. Dividing its emphasis between distributed model training, real-time inference, and automated lifecycle management, this paper explores how cloud-native technology acts to enable large-scale AI-based predictive modeling. By integrating MLOps practices with elastic cloud infrastructure, organizations will be able to realize better fault tolerance, faster deployment schedules, and the most efficient use of resources. The proposed methodology demonstrates that cloud-native ideas can help AI systems work with a vast amount of data, dynamically adapt to changing loads, and maintain high performance levels in the actual environment. Keywords: Cloud-native architecture, predictive modeling, containerization, MLOps, microservices, real-time inference, serverless computing.},
  copyright = {Copyright (c) 2025 Journal of Emerging Technology and Digital Transformation},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/UDHVIGCD/Bajwa et al. - 2025 - CLOUD-NATIVE ARCHITECTURES FOR LARGE-SCALE AI-BASED PREDICTIVE MODELING.pdf}
}

@inproceedings{daschowdaryDevOps20Embracing2024,
  title = {{{DevOps}} 2.0: {{Embracing AI}}/{{ML}}, {{Cloud-Native Development}}, and a {{Culture}} of {{Continuous Transformation}}},
  shorttitle = {{{DevOps}} 2.0},
  booktitle = {2024 4th {{International Conference}} on {{Pervasive Computing}} and {{Social Networking}} ({{ICPCSN}})},
  author = {Das Chowdary, V. Hemanth and Shanmukh, A. and Nikhil, T. Phaneendra and Kumar, B. Sandeep and Khan, Feroz},
  year = {2024},
  month = may,
  pages = {673--679},
  doi = {10.1109/ICPCSN62568.2024.00112},
  urldate = {2025-11-04},
  abstract = {This research study investigates the emerging domain of DevOps, assessing its capacity to fundamentally transform the processes of software development and delivery. A notable advancement in the field of DevOps is the amalgamation of AI/ML and artificial intelligence. These technologies possess significant potential to revolutionize the DevOps domain through the automation of tedious duties, including provisioning infrastructure, managing configurations, and identifying anomalies. This study highlights the increasing need for streamlined solutions. By harnessing the capabilities of AI and ML-driven tools, DevOps teams will have the ability to streamline their processes, enabling them to allocate resources to more strategic approaches such as nurturing innovation, addressing complex challenges, and making decisions based on data. This study will investigate the precise methods by which AI/ML can enhance DevOps practices, resulting in a software development environment that is more secure, efficient, and agile.},
  keywords = {Artificial intelligence (AI),Automation,Cloud computing,Containerization,Continuous delivery,Cultural transformation,DevOps,Industries,Machine learning (ML),Microservice architectures,Microservices,Social networking (online),Software development,Technological innovation,Transforms},
  file = {/Users/nilsarnold/Zotero/storage/2FXEKUUE/Das Chowdary et al. - 2024 - DevOps 2.0 Embracing AIML, Cloud-Native Development, and a Culture of Continuous Transformation.pdf}
}

@article{desmondConvergenceAIDevOps2024,
  title = {The {{Convergence}} of {{AI}} and {{DevOps}}: {{Exploring Adaptive Automation}} and {{Proactive System Reliability}}},
  author = {Desmond, Osinaka Chukwu},
  year = {2024},
  volume = {12},
  number = {9},
  abstract = {The convergence of AI with DevOps introduces progressive automatization and increases preventive system dependability in modern software construction. Many initial DevOps practices centred around continuous integration, delivery and deployment models have limits when dealing with more complex and distributed systems. AI can revolutionize the DevOps approach by automating mundane tasks, enhancing the team's productivity, and providing solutions to avoid system failure that may result in downtime. Indeed, by correlating Derivative events with the relevant Root Causes and analyzing patterns in and performances of computer systems, this paper examines how various forms of AI -- including predictive analytics and 'self-healing' techniques -- can raise DevOps processes to new levels of intelligence and optimization. The work introduces a new concept of employing autonomous ML models to maintain always-on delivery pipelines and prevent system failures, thus demonstrating the effects of AI on deployment frequency, mean time to recovery, and failure rate. Comparisons are also made to standard DevOps frameworks, as the paper highlights how integrating artificial intelligence enhances the applicability to large-scale automation and secure reliability.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/Q2VMBKML/Desmond - 2024 - The Convergence of AI and DevOps Exploring Adaptive Automation and Proactive System Reliability.pdf}
}

@article{enemosahEnhancingDevOpsEfficiency2025a,
  title = {Enhancing {{DevOps Efficiency}} through {{AI-Driven Predictive Models}} for {{Continuous Integration}} and {{Deployment Pipelines}}},
  author = {Enemosah, Aliyu},
  year = {2025},
  month = jan,
  journal = {International Journal of Research Publication and Reviews},
  volume = {Vol 6},
  pages = {871--887},
  doi = {10.55248/gengpi.6.0125.0229},
  abstract = {The adoption of Artificial Intelligence (AI) in DevOps workflows has transformed traditional Continuous Integration and Deployment (CI/CD) pipelines by enabling predictive modelling to enhance efficiency, reliability, and scalability. As modern software systems grow in complexity, the need for intelligent automation to optimize CI/CD processes has become critical. This paper investigates the integration of AI-driven predictive models in DevOps pipelines, focusing on their ability to forecast build failures, optimize resource allocation, and streamline testing and deployment cycles. The study explores various AI techniques, including machine learning algorithms like regression, clustering, and neural networks, to address specific challenges in CI/CD processes. Predictive models trained on historical pipeline data can identify patterns, detect anomalies, and recommend proactive actions to prevent bottlenecks and failures. Additionally, the use of reinforcement learning enables dynamic resource management, ensuring efficient scaling during peak workloads. Key case studies illustrate the application of AI-driven predictive models in optimizing Jenkins and GitLab pipelines, achieving significant reductions in build times and improving deployment success rates. The research also highlights the role of AI in prioritizing test cases, automating performance monitoring, and enhancing feedback loops for continuous improvement. While emphasizing the benefits of AI integration, this paper also addresses challenges such as data quality, algorithm selection, and organizational readiness for adopting intelligent systems. By synthesizing these advancements, the paper provides a roadmap for leveraging AI to revolutionize DevOps workflows, paving the way for faster, more reliable software delivery in dynamic environments.},
  file = {/Users/nilsarnold/Zotero/storage/XY78Z844/Enemosah - 2025 - Enhancing DevOps Efficiency through AI-Driven Predictive Models for Continuous Integration and Deplo.pdf}
}

@book{govindarajanMachineLearningBased2025,
  title = {Machine {{Learning Based Approach}} for {{Handling Imbalanced Data}} for {{Intrusion Detection}} in the {{Cloud Environment}}},
  author = {Govindarajan, Vijay},
  year = {2025},
  month = mar,
  pages = {815},
  doi = {10.1109/ICDT63985.2025.10986614},
  file = {/Users/nilsarnold/Zotero/storage/G7JD83PL/Govindarajan - 2025 - Machine Learning Based Approach for Handling Imbalanced Data for Intrusion Detection in the Cloud En.pdf}
}

@article{guptaCloudNativeMLArchitecting2024a,
  title = {Cloud-{{Native ML}}: {{Architecting AI Solutions}} for {{Cloud-First Infrastructures}}},
  shorttitle = {Cloud-{{Native ML}}},
  author = {Gupta, Abhishek and Chaturvedi, Yashovardhan},
  year = {2024},
  month = dec,
  journal = {Nanotechnology Perceptions},
  volume = {20},
  pages = {930--939},
  doi = {10.62441/nano-ntp.v20i7.4004},
  abstract = {The integration of cloud-native architectures into artificial intelligence (AI) workflows has revolutionized the deployment, scalability, and efficiency of machine learning (ML) solutions. This study explores the design and evaluation of cloud-native ML models within cloud-first infrastructures, emphasizing their performance, cost-effectiveness, and scalability. Leveraging platforms such as AWS, Google Cloud, and Microsoft Azure, the research investigates data pipeline efficiency, model training metrics, inference performance, and economic viability. Statistical analyses reveal consistent accuracy, precision, and recall across models, with distinct trade-offs in resource utilization and latency between batch and real-time inference methods. The findings highlight the transformative potential of cloud-native ML in optimizing AI-driven decision-making, while identifying challenges such as resource allocation and cost management. This study serves as a foundation for advancing AI applications in cloud environments, offering insights for organizations to achieve greater agility and efficiency in AI deployment.}
}

@article{joshiReviewDataPipelines2025,
  title = {Review of {{Data Pipelines}} and {{Streaming}} for {{Generative AI Integration}}: {{Challenges}}, {{Solutions}}, and {{Future Directions}}},
  shorttitle = {Review of {{Data Pipelines}} and {{Streaming}} for {{Generative AI Integration}}},
  author = {Joshi, Satyadhar},
  year = {2025},
  month = feb,
  journal = {International Journal of Research Publication and Reviews},
  volume = {6},
  pages = {2348--2357},
  doi = {10.55248/gengpi.6.0225.0919},
  abstract = {This work discussed the role of real-time data pipelines in powering GenAI applications. We synthesize existing literature, categorizing it into key areas: data integration, streaming platforms, vector databases, and architectural patterns. We discuss the challenges and opportunities in building robust and scalable real-time data pipelines for GenAI, emphasizing the importance of data freshness, accuracy, and efficient processing. This work provides a valuable overview for practitioners and researchers seeking to leverage real-time data for enhanced GenAI capabilities. The intersection of generative artificial intelligence (GenAI) and big data infrastructure has led to novel data management techniques, including data streaming, integration, and vector databases. This paper explores these techniques, their applications, and the critical role of data pipelines in optimizing AI-driven decision-making. We survey contemporary methodologies and highlight future challenges and opportunities in deploying real-time GenAI applications. The integration of data streaming platforms with generative AI (GenAI) has emerged as a critical area of research, enabling real-time data processing and enhancing AI applications. This paper reviews the current state of the art, focusing on the role of technologies like Apache Kafka, vector databases, and cloud-based solutions in addressing challenges such as data freshness, scalability, and integration complexity. We also explore future directions, including the use of retrieval-augmented generation (RAG) and real-time data pipelines, to unlock the full potential of GenAI. This review synthesizes insights from recent studies, industry practices, and emerging trends to provide a comprehensive understanding of the field.}
}

@article{kankanalaAIMLDevOps2024,
  title = {{{AI}}/{{ML}} -- {{DevOps Automation}}},
  author = {Kankanala, Giridhar and Amgothu, Sudheer},
  year = {2024},
  month = oct,
  volume = {13},
  pages = {111--117},
  abstract = {The rapid advancement of Artificial Intelligence (AI) and Machine Learning (ML) is significantly transforming DevOps, particularly in cloud environments, where the need for speed, security, and scalability is paramount. This research investigates how AI/ML enhances automation within DevOps workflows, improves threat detection capabilities, and optimizes workload distribution in cloud-native environments. By integrating AI/ML into Continuous Integration/Continuous Deployment (CI/CD) pipelines, we aim to improve operational efficiency and detect threats with greater precision. We also evaluate the performance of AI-based workload optimization in dynamically scaling resources, reducing latency, and minimizing resource wastage. Our experimental results demonstrate that AI-powered systems can reduce deployment times by up to 30\%, improve threat detection rates by 45\%, and significantly enhance resource management compared to traditional systems.},
  file = {/Users/nilsarnold/Zotero/storage/9WYXFTMW/Kankanala und Amgothu - 2024 - AIML â€“ DevOps Automation.pdf}
}

@article{karthikputhrayaRoleCloudNativeArchitectures2025,
  title = {The {{Role}} of {{Cloud-Native Architectures}} in {{Accelerating Machine Learning Workflows}} through {{Data Engineering Innovations}}},
  author = {Karthik Puthraya and Rachit Gupta and Beverly DSouza},
  year = {2025},
  month = feb,
  publisher = {SARC Publisher},
  issn = {2945-3437},
  doi = {10.5281/ZENODO.15106432},
  urldate = {2025-11-05},
  abstract = {The rapid advancement of machine learning (ML) necessitates scalable, efficient, and cost-effective computing environments. Traditional ML workflows often face challenges related to long training times, high inference latency, infrastructure costs, and scalability limitations. This study explores the role of cloud-native architectures in accelerating ML workflows through data engineering innovations. By leveraging microservices, containerization, serverless computing, and automated data pipelines, cloud-native environments optimize ML operations while reducing computational overhead. The results indicate a 50\% reduction in training time and inference latency, 50-55\% cost savings, and a threefold increase in scalability compared to traditional ML implementations. Moreover, cloud-native solutions enhance fault tolerance by reducing system recovery time by 80\%, ensuring greater reliability for real-time AI applications. Statistical analyses, including regression modeling, survival analysis, and PCA, confirm the efficiency gains of cloud-based ML workflows. The findings suggest that organizations adopting cloud-native ML architectures can achieve faster model deployment, reduced infrastructure costs, and enhanced system resilience. As cloud-native computing evolves, its integration with machine learning will play a pivotal role in shaping future AI-driven solutions.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/2GBV73NT/Karthik Puthraya et al. - 2025 - The Role of Cloud-Native Architectures in Accelerating Machine Learning Workflows through Data Engin.pdf}
}

@article{kathiresanCybersecurityRiskModeling2025,
  title = {Cybersecurity {{Risk Modeling}} in {{CI}}/{{CD Pipelines Using Reinforcement Learning}} for {{Test Optimization}}},
  author = {Kathiresan, Gopinath},
  year = {2025},
  month = may,
  journal = {International Journal of Innovative Science and Research Technology},
  pages = {15--25},
  doi = {10.38124/ijisrt/25may339},
  urldate = {2025-11-03},
  abstract = {Incremental software development and deployment brought about the much-advertised Continuous Integration and Continuous Deployment (CI/CD) approaches that have changed completely how modern applications are constructed, tested, and launched. But the fast-delivery strategy hugely opened the gates to cyber threats, giving CI/CD pipelines the status of most-sought cyber-hacking targets. Traditional static security models have been frequently experienced to fail in in line with the dynamic nature of CI/CD workflows, hence allowing undetected vulnerabilities to persist and prolonging remediation. This study proposes the utilization of reinforcement learning (RL) for optimizing cybersecurity risk modeling and testing in CI/CD pipelines. The system makes maximum use of real-time threat intelligence, in combination with dynamic test selection techniques, toward maximum detection of vulnerabilities within the smallest possible amount of resource allocation. RL agents are trained to always push severe test scenarios first in a way to better absorb changing attacks and codebase dynamics. Empirical study results show improved detection rates, less test time, and better risk visibility in all stages of the pipeline, marking a major fight toward intelligent and adaptive DevOps security practices.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/3BR9GY3W/Kathiresan - 2025 - Cybersecurity Risk Modeling in CICD Pipelines Using Reinforcement Learning for Test Optimization.pdf}
}

@article{khademChallengesMetricsLLMdriven2025,
  title = {From Challenges to Metrics: {{An LLM-driven DevOps}} Recommendation System Grounded in Evidence-Based Mappings},
  shorttitle = {From Challenges to Metrics},
  author = {Khadem, Ehsan Azizi and Movaghar, Ali},
  year = {2025},
  month = dec,
  journal = {Array},
  volume = {28},
  pages = {100547},
  issn = {25900056},
  doi = {10.1016/j.array.2025.100547},
  urldate = {2025-11-03},
  abstract = {DevOps is widely adopted, yet many organizations continue to face barriers such as integration gaps, unclear objectives, and fragmented decision support. These barriers limit the alignment of DevOps initiatives with organizational goals. This study introduces an empirically grounded recommendation system that connects DevOps adoption challenges to success factors, operational processes, and performance metrics. The system is built on a structured mapping model and uses a semantic recommendation engine enhanced with large language model capabilities. It interprets natural-language descriptions of challenges and generates context-aware recommendations by combining semantic retrieval with evidence from 378 peer-reviewed studies. Evaluation shows that the system outperforms keyword-based retrieval, achieving higher precision at rank three and improved expert-rated relevance. By linking practitioner challenges with systematically curated evidence, the approach provides actionable and auditable guidance for DevOps transformation planning. The results highlight the potential of semantic reasoning and structured knowledge extraction to support evidence-based decision-making in software engineering practice.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/7FEINFHN/Khadem und Movaghar - 2025 - From challenges to metrics An LLM-driven DevOps recommendation system grounded in evidence-based ma.pdf}
}

@article{kreuzbergerMachineLearningOperations2023,
  title = {Machine {{Learning Operations}} ({{MLOps}}): {{Overview}}, {{Definition}}, and {{Architecture}}},
  shorttitle = {Machine {{Learning Operations}} ({{MLOps}})},
  author = {Kreuzberger, Dominik and K{\"u}hl, Niklas and Hirschl, Sebastian},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {31866--31879},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3262138},
  urldate = {2025-11-03},
  abstract = {The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we contribute to the body of knowledge by providing an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we provide a comprehensive definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.},
  keywords = {Automation,Bibliographies,CI/CD,Codes,Collaboration,DevOps,Interviews,machine learning,Machine learning,MLOps,operations,Training,workflow orchestration},
  file = {/Users/nilsarnold/Zotero/storage/C6U8VXWG/Kreuzberger et al. - 2023 - Machine Learning Operations (MLOps) Overview, Definition, and Architecture.pdf}
}

@misc{MLOpsApproachCloudnative,
  title = {{{MLOps}} Approach in the Cloud-Native Data Pipeline Design {\textbar} {{Acta Technica Jaurinensis}}},
  urldate = {2025-11-04},
  howpublished = {https://acta.sze.hu/index.php/acta/article/view/581},
  file = {/Users/nilsarnold/Zotero/storage/G97Y4MFY/MLOps approach in the cloud-native data pipeline design  Acta Technica Jaurinensis.pdf}
}

@article{murthyAIPoweredPredictiveScaling2021,
  title = {{{AI-Powered Predictive Scaling}} in {{Cloud Computing}}: {{Enhancing Efficiency}} through {{Real-Time Workload Forecasting}}},
  author = {Murthy, Pranav and Bobba, Sundeep},
  year = {2021},
  volume = {5},
  number = {4},
  abstract = {AI-powered predictive scaling in cloud computing leverages machine learning algorithms to anticipate future workload demands and optimize resource allocation accordingly. Unlike traditional scaling methods that react to changes in demand, predictive scaling proactively adjusts resources based on predictions derived from historical and real-time data. This approach offers significant benefits, including improved resource utilization, cost savings, and enhanced system performance. However, it also faces challenges such as data quality issues, algorithm limitations, and integration complexities. As AI and machine learning technologies continue to advance, predictive scaling is expected to evolve, integrating with emerging technologies and adapting to diverse cloud environments. This paper explores the mechanisms of predictive scaling, its benefits and challenges, and future trends in its development.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/38RBKK3W/Murthy und Bobba - 2021 - AI-Powered Predictive Scaling in Cloud Computing Enhancing Efficiency through Real-Time Workload Fo.pdf}
}

@article{oyekunleclaudiusoyeniranAIdrivenDevopsLeveraging2023,
  title = {{{AI-driven}} Devops: {{Leveraging}} Machine Learning for Automated Software Deployment and Maintenance},
  shorttitle = {{{AI-driven}} Devops},
  author = {{Oyekunle Claudius Oyeniran} and {Adebunmi Okechukwu Adewusi} and {Adams Gbolahan Adeleke} and {Lucy Anthony Akwawa} and {Chidimma Francisca Azubuko}},
  year = {2023},
  month = dec,
  journal = {Engineering Science \& Technology Journal},
  volume = {4},
  number = {6},
  pages = {728--740},
  issn = {2708-8952, 2708-8944},
  doi = {10.51594/estj.v4i6.1552},
  urldate = {2025-11-04},
  abstract = {The integration of artificial intelligence (AI) and machine learning (ML) into DevOps practices is revolutionizing software deployment and maintenance, paving the way for more efficient, reliable, and scalable systems. Traditional DevOps, characterized by continuous integration and continuous delivery (CI/CD), often struggles with scalability, error-prone processes, and the need for constant human oversight. AI-driven DevOps introduces intelligent automation, enabling predictive analytics, anomaly detection, and self-healing infrastructure. By leveraging AI/ML, organizations can predict deployment outcomes, identify potential issues in real time, and automatically rectify them, reducing downtime and enhancing overall system performance. This paper explores the current state of DevOps, highlighting its limitations and the transformative potential of AI/ML integration. We discuss key AI/ML use cases in DevOps, such as automated code quality analysis, predictive analytics for deployment, and self-healing systems. Additionally, we examine the tools and technologies that facilitate AI-driven DevOps, including ML frameworks like TensorFlow and observability platforms like Datadog. Despite its potential, AI-driven DevOps faces challenges, including data quality, integration complexity, and ethical considerations. The paper also looks into the future of AI in DevOps, envisioning a fully autonomous deployment and maintenance ecosystem. By addressing current challenges and embracing AI/ML technologies, organizations can significantly improve their DevOps processes, leading to faster, more reliable software delivery.  Keywords: AI-driven DevOps, Machine Learning, Automated Software Deployment, Continuous Integration, Continuous Delivery, Predictive Analytics.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/5TDMNTG5/Oyekunle Claudius Oyeniran et al. - 2023 - AI-driven devops Leveraging machine learning for automated software deployment and maintenance.pdf}
}

@article{poudelAIDrivenIntelligentAutoScaling2025a,
  title = {{{AI-Driven Intelligent Auto-Scaling}} for {{Cloud Resource Optimization}} 1},
  author = {Poudel, Sudip and Marasini, Kushal and Bhatt, Laxmi and Baral, Daya},
  year = {2025},
  month = oct,
  journal = {Journal of Advanced College of Engineering and Management},
  volume = {Vol. 11},
  pages = {27--36},
  doi = {10.3126/jacem.v11i1.84521},
  abstract = {This study introduces a predictive, AI-powered auto-scaling framework designed to optimize resource usage in cloud environments, specifically within Amazon Web Services (AWS). Conventional rule-based scaling methods often result in inefficiencies, either wasting resources or degrading performance. To overcome these challenges, this work employs Long Short-Term Memory (LSTM) neural networks that analyze historical performance data collected from AWS CloudWatch. The system forecasts resource demand trends for EC2 and RDS instances and automates scaling actions using the Boto3 SDK. It evaluates multiple metrics-including CPU usage, memory availability, disk I/O, and network traffic-to make accurate, real-time decisions. Operating in a continuous loop, the model updates hourly to adapt to changing workloads. Experimental evaluation confirms that the proposed approach reduces operational costs and enhances performance reliability. This research delivers a scalable, intelligent solution for cloud resource management, suitable for dynamic application environments where responsiveness and efficiency are critical.},
  file = {/Users/nilsarnold/Zotero/storage/UYY9B9UJ/Poudel et al. - 2025 - AI-Driven Intelligent Auto-Scaling for Cloud Resource Optimization 1.pdf}
}

@article{reddygopireddyIntegratingAIDevOps2022,
  title = {Integrating {{AI}} into {{DevOps}}: {{Leveraging Machine Learning}} for {{Intelligent Automation}} in {{Azure}}},
  shorttitle = {Integrating {{AI}} into {{DevOps}}},
  author = {Reddy Gopireddy, Satheesh},
  year = {2022},
  month = jun,
  journal = {International Journal of Science and Research (IJSR)},
  volume = {11},
  number = {6},
  pages = {2035--2039},
  issn = {23197064},
  doi = {10.21275/SR22619111757},
  urldate = {2025-11-03},
  abstract = {The convergence of Artificial Intelligence (AI) and DevOps is paving the way for transformative changes in the way software is developed, deployed, and managed, particularly in cloud environments like Microsoft Azure. By integrating AI, specifically Machine Learning (ML), into DevOps practices, organizations can achieve intelligent automation, predictive analytics, anomaly detection, and continuous optimization. This paper explores how AI can be effectively leveraged within Azure's DevOps ecosystem, offering insights into the benefits, challenges, and best practices associated with this integration. The article delves into case studies, provides strategic frameworks, and discusses future trends that will shape the evolution of AI-driven DevOps.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/7Q949NIR/Reddy Gopireddy - 2022 - Integrating AI into DevOps Leveraging Machine Learning for Intelligent Automation in Azure.pdf}
}

@article{satyadharjoshiIntroductionGenerativeAI2025,
  title = {Introduction to {{Generative AI}} and {{DevOps}}: {{Synergies}}, {{Challenges}} and {{Applications}}},
  shorttitle = {Introduction to {{Generative AI}} and {{DevOps}}},
  author = {{Satyadhar Joshi}},
  year = {2025},
  month = mar,
  journal = {International Journal of Advanced Research in Science, Communication and Technology},
  pages = {205--225},
  issn = {2581-9429},
  doi = {10.48175/IJARSCT-23634},
  urldate = {2025-11-03},
  abstract = {This paper provides a comprehensive review of the applications of Generative AI in DevOps, analyzing recent advancements, methodologies, and challenges. We examine key contributions from the literature and discuss the future trajectory of AI-driven automation in DevOps workflows. As Software development continues to evolve, and the demand for automation tools that can learn and adapt on their own grows. Generative AI has revolutionized various industries, including DevOps, by enabling the creation of new content, from text and images to music and code. In this paper, we explore the impact of generative AI in DevOps, its applications, and the future prospects. This paper reviews the emerging trends and applications of Generative AI in DevOps, examining its impact on automation, CI/CD pipelines, Kubernetes management, and overall efficiency. We explore the concept of "GenOps" and the use of containers for deploying AI applications, highlighting key research and developments in this rapidly evolving field. We explore the transformative impact of AI agents, containerization, and automation tools on software development and operations. The review covers various aspects, including AI-driven code generation, infrastructure management, and continuous delivery pipelines, highlighting the potential of Generative AI to enhance efficiency and productivity in modern DevOps environments. We review recent advancements, tools, and methodologies that leverage Generative AI to optimize DevOps pipelines and vice versa. The paper also discusses challenges and future directions for integrating Generative AI into DevOps workflows.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/ECLLYF7I/Satyadhar Joshi - 2025 - Introduction to Generative AI and DevOps Synergies, Challenges and Applications.pdf}
}

@article{sikhaCloudNativeApplicationDevelopment2023,
  title = {Cloud-{{Native Application Development}} for {{AI- Conducive Architectures}}.},
  author = {Sikha, Vijay Kartik},
  year = {2023},
  journal = {International Journal on Recent and Innovation Trends in Computing and Communication},
  volume = {11},
  number = {11},
  abstract = {The integration of AI technologies into cloud-native application development is reshaping the landscape of software engineering by enhancing scalability, flexibility, and efficiency. This paper explores the evolution of development processes driven by advancements in AI and cloud technologies, emphasizing the importance of adopting an AI mindset. It discusses how cloud-native architectures, characterized by microservices and containerization, align well with AI requirements, enabling organizations to deploy and scale AI-driven applications effectively. Key guidelines for balancing innovation with cost-efficiency are outlined, including leveraging generative AI tools, optimizing resource management, and implementing observability practices. The paper also examines the impact of AI on architectural decision-making, such as data management and real-time processing, and highlights the role of databases in managing extensive data required for AI tasks. Additionally, the paper compares legacy tools with AI-driven tools in accelerating development processes and addresses security considerations in AI-enabled cloud-native applications. The future direction of AI-driven cloud-native development is discussed, focusing on advancements in efficiency, scalability, and security.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/3F5K73RF/Sikha - 2023 - Cloud-Native Application Development for AI- Conducive Architectures..pdf}
}

@article{supritpattanayakIntegratingAIDevOps2024,
  title = {Integrating {{AI}} into {{DevOps}} Pipelines: {{Continuous}} Integration, Continuous Delivery, and Automation in Infrastructural Management: {{Projections}} for Future},
  shorttitle = {Integrating {{AI}} into {{DevOps}} Pipelines},
  author = {{Suprit Pattanayak} and {Pranav Murthy} and {Aditya Mehra}},
  year = {2024},
  month = oct,
  journal = {International Journal of Science and Research Archive},
  volume = {13},
  number = {1},
  pages = {2244--2256},
  issn = {25828185},
  doi = {10.30574/ijsra.2024.13.1.1838},
  urldate = {2025-11-03},
  abstract = {This paper explores the transformative role of Artificial Intelligence (AI) in enhancing DevOps pipelines, focusing on Continuous Integration (CI), Continuous Delivery (CD), and automation in infrastructural management. As organizations increasingly adopt DevOps methodologies to streamline software development and delivery processes, the integration of AI technologies offers significant potential for improving efficiency, quality, and responsiveness. This study reviews current practices in CI/CD and examines how AI-driven tools can automate repetitive tasks, optimize resource allocation, and facilitate predictive analytics for proactive decision-making. Additionally, the paper discusses the challenges and considerations associated with AI integration in DevOps, including cultural shifts, data governance, and the need for skilled personnel. Projections for future developments highlight the potential for AI to create smarter, more adaptive DevOps environments that align with evolving industry demands. By identifying key trends and innovations, this research provides a comprehensive overview of the future landscape of DevOps, positioning AI as a critical enabler of agility and performance.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/8T3VMWHP/Suprit Pattanayak et al. - 2024 - Integrating AI into DevOps pipelines Continuous integration, continuous delivery, and automation in.pdf}
}

@inproceedings{symeonidisMLOpsDefinitionsTools2022,
  title = {{{MLOps}} - {{Definitions}}, {{Tools}} and {{Challenges}}},
  booktitle = {2022 {{IEEE}} 12th {{Annual Computing}} and {{Communication Workshop}} and {{Conference}} ({{CCWC}})},
  author = {Symeonidis, Georgios and Nerantzis, Evangelos and Kazakis, Apostolos and Papakostas, George A.},
  year = {2022},
  month = jan,
  pages = {0453--0460},
  doi = {10.1109/CCWC54503.2022.9720902},
  urldate = {2025-11-05},
  abstract = {This paper is an concentrated overview of the Machine Learning Operations (MLOps) area. Our aim is to define the operation and the components of such systems by highlighting the current problems and trends. In this context we present the different tools and their usefulness in order to provide the corresponding guidelines. Moreover, the connection between MLOps and AutoML (Automated Machine Learning) is identified and how this combination could work is proposed. The novelty of our approach relies on the combination of state-of-the-art topics such as AutoML, exlainability and sustain-ability in order to overcome the current challenges in MLOps identifying them not only as the answer for the incorporation of ML models in production but also as a possible tool for efficient, robust and accurate machine learning models.},
  keywords = {AutoML,Computational modeling,Conferences,Deployment,explainability,fairness,machine learning,Machine learning,Market research,MLOps,monitoring,Production,re-training,robustness,Robustness,sustainability,Training},
  file = {/Users/nilsarnold/Zotero/storage/3S6LYCEH/MLOps_-_Definitions_Tools_and_Challenges.pdf}
}

@article{tamanampudiAIEnhancedContinuousIntegration,
  title = {{{AI-Enhanced Continuous Integration}} and {{Continuous Deployment Pipelines}}: {{Leveraging Machine Learning Models}} for {{Predictive Failure Detection}}, {{Automated Rollbacks}}, and {{Adaptive Deployment Strategies}} in {{Agile Software Development}}},
  author = {Tamanampudi, Venkata Mohit},
  volume = {10},
  abstract = {The integration of artificial intelligence (AI) and machine learning (ML) into Continuous Integration and Continuous Deployment (CI/CD) pipelines has the potential to significantly enhance the agility, reliability, and efficiency of software development processes. This research paper investigates the application of AI-enhanced methodologies within CI/CD pipelines, focusing on how machine learning models can be utilized to address core challenges in agile software development, particularly in the domains of predictive failure detection, automated rollbacks, and adaptive deployment strategies. The study posits that by embedding intelligent systems into CI/CD workflows, software teams can mitigate risks, reduce downtime, and achieve more reliable and faster releases, while simultaneously improving overall software quality.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/6CG7I9M7/Tamanampudi - AI-Enhanced Continuous Integration and Continuous Deployment Pipelines Leveraging Machine Learning.pdf}
}

@article{tamminediAutomatingKubernetesOperations2024,
  title = {Automating {{Kubernetes Operations}} with {{AI}} and {{Machine Learning}}},
  author = {Tamminedi, Varun},
  year = {2024},
  month = dec,
  journal = {IJFMR - International Journal For Multidisciplinary Research},
  volume = {6},
  number = {6},
  publisher = {IJFMR},
  issn = {2582-2160},
  doi = {10.36948/ijfmr.2024.v06i06.33430},
  urldate = {2025-11-03},
  copyright = {Creative Commons Attribution-ShareAlike 4.0 International License},
  file = {/Users/nilsarnold/Zotero/storage/95KDISVQ/Tamminedi - 2024 - Automating Kubernetes Operations with AI and Machine Learning.pdf}
}

@article{uddohAIBasedThreatDetection2021,
  title = {{{AI-Based Threat Detection Systems}} for {{Cloud Infrastructure}}: {{Architecture}}, {{Challenges}}, and {{Opportunities}}},
  shorttitle = {{{AI-Based Threat Detection Systems}} for {{Cloud Infrastructure}}},
  author = {Uddoh, Jeanette and Ajiga, Daniel and Okare, Babawale Patrick and Aduloju, Tope David},
  year = {2021},
  journal = {Journal of Frontiers in Multidisciplinary Research},
  volume = {2},
  number = {2},
  pages = {61--67},
  issn = {30509718, 30509726},
  doi = {10.54660/.IJFMR.2021.2.2.61-67},
  urldate = {2025-11-04},
  abstract = {The rapid adoption of cloud infrastructure has transformed organizational operations, offered scalability and flexibility but also exposed enterprises to sophisticated cyber threats, such as data breaches, ransomware, and insider attacks. AI-based threat detection systems have emerged as a critical solution, leveraging machine learning, deep learning, and behavioral analytics to identify and mitigate threats in real time. This paper proposes novel architecture for AI-based threat detection in cloud infrastructure, addressing the unique challenges of dynamic, distributed environments. Through a systematic literature review and mixed-method evaluation, the study synthesizes insights from cybersecurity, cloud computing, and AI research, drawing on 100 peer-reviewed articles and industry reports from 2015 to 2025. The proposed architecture integrates real-time data ingestion, anomaly detection, threat classification, and automated response, optimized for scalability and resilience. Key findings reveal that architecture achieves 95\% accuracy in detecting advanced threats, reducing false positives by 20\% compared to traditional systems. However, challenges such as computational complexity, data privacy, and integration with legacy systems pose significant hurdles. Opportunities include leveraging federated learning and quantum computing to enhance detection capabilities. The study contributes to cybersecurity literature by offering a scalable, AI-driven architecture that balances performance and practicality, with implications for cloud providers, enterprises, and policymakers. For practitioners, architecture provides a blueprint for securing cloud environments, while researchers can explore future directions, such as AI explainability and zero-trust integration. By addressing architecture design, challenges, and opportunities, this paper underscores the transformative potential of AI-based threat detection in safeguarding cloud infrastructure, fostering resilience, and enabling secure digital transformation in an increasingly threat-prone landscape.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/CKZTISVG/Uddoh et al. - 2021 - AI-Based Threat Detection Systems for Cloud Infrastructure Architecture, Challenges, and Opportunit.pdf}
}
