@article{amteCloudNativeAIChallenges2025,
  title = {Cloud-{{Native AI}}: {{Challenges}} and {{Innovations}} in {{Deploying Large-Scale Machine Learning Models}}},
  shorttitle = {Cloud-{{Native AI}}},
  author = {Amte, Rahul},
  year = 2025,
  month = mar,
  journal = {ISCSITR - INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING (ISCSITR-IJSRAIML) ISSN (Online): 3067-753X},
  volume = {6},
  number = {2},
  pages = {9--18},
  urldate = {2025-11-05},
  abstract = {Large scale machine learning models are deployed with revolutionary speed in cloud native AI for the reasons of scalability, flexibility and cost efficiency. However, most of these challenges face a latency problem, resource management issue, and most of all security risk. This paper considers challenges in producing cloud native AI, such as containerization, microservices, and optimized inference pipelines and then it studies innovations in these areas. By qualitative and quantitative analysis, we examine best practices of deploying AI models effectively in cloud environment. The results indicate ways of cost effective and high-performance AI implementation, with a focus on automation, edge computation, and serverless hands in increasing the deployment of AI on a sustainable basis.},
  copyright = {Copyright (c) -1 ISCSITR - INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN  ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING (ISCSITR-IJSRAIML)},
  langid = {english},
  keywords = {AI,Cloud,Deployment,Machine Learning},
  file = {/Users/nilsarnold/Zotero/storage/63CZ2GJP/Amte - 2025 - Cloud-Native AI Challenges and Innovations in Deploying Large-Scale Machine Learning Models.pdf}
}

@misc{atlassianWasIstDevOps,
  title = {{Was ist DevOps?}},
  author = {Atlassian},
  journal = {Atlassian},
  urldate = {2025-12-16},
  abstract = {DevOps ist eine Partnerschaft zwischen Softwareentwicklung und IT-Operations, bei der Kommunikation, Zusammenarbeit und Integration im Mittelpunkt stehen.},
  howpublished = {https://www.atlassian.com/de/devops},
  langid = {ngerman},
  file = {/Users/nilsarnold/Zotero/storage/HNDQ9NL9/devops.html}
}

@article{bajwaCLOUDNATIVEARCHITECTURESLARGESCALE2025,
  title = {{{CLOUD-NATIVE ARCHITECTURES FOR LARGE-SCALE AI-BASED PREDICTIVE MODELING}}},
  author = {Bajwa, Muhammad Talha Tahir and Wattoo, Saman and Mehmood, Irum and Talha, Muhammad and Anwar, Muhammad Junaid and Ullah, Muhammad Sana},
  year = 2025,
  month = aug,
  journal = {Journal of Emerging Technology and Digital Transformation},
  volume = {4},
  number = {2},
  pages = {207--221},
  issn = {3006-9726},
  urldate = {2025-10-25},
  abstract = {The demand of adapted, expandable, efficient deployment techniques has become more acknowledged because of the accelerated growth of artificial intelligence (AI) initiatives and high intricacity of big forms of predictive modeling. Cloud-native architectures which are founded on concepts such as serverless computing, microservices, orchestration and containerization create a solid foundation in satisfying these needs. Dividing its emphasis between distributed model training, real-time inference, and automated lifecycle management, this paper explores how cloud-native technology acts to enable large-scale AI-based predictive modeling. By integrating MLOps practices with elastic cloud infrastructure, organizations will be able to realize better fault tolerance, faster deployment schedules, and the most efficient use of resources. The proposed methodology demonstrates that cloud-native ideas can help AI systems work with a vast amount of data, dynamically adapt to changing loads, and maintain high performance levels in the actual environment. Keywords: Cloud-native architecture, predictive modeling, containerization, MLOps, microservices, real-time inference, serverless computing.},
  copyright = {Copyright (c) 2025 Journal of Emerging Technology and Digital Transformation},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/UDHVIGCD/Bajwa et al. - 2025 - CLOUD-NATIVE ARCHITECTURES FOR LARGE-SCALE AI-BASED PREDICTIVE MODELING.pdf}
}

@article{enemosahEnhancingDevOpsEfficiency2025,
  title = {Enhancing {{DevOps Efficiency}} through {{AI-Driven Predictive Models}} for {{Continuous Integration}} and {{Deployment Pipelines}}},
  author = {Enemosah, Aliyu},
  year = 2025,
  month = jan,
  journal = {International Journal of Research Publication and Reviews},
  volume = {6},
  number = {1},
  pages = {871--887},
  issn = {25827421},
  doi = {10.55248/gengpi.6.0125.0229},
  urldate = {2025-11-03},
  abstract = {The adoption of Artificial Intelligence (AI) in DevOps workflows has transformed traditional Continuous Integration and Deployment (CI/CD) pipelines by enabling predictive modelling to enhance efficiency, reliability, and scalability. As modern software systems grow in complexity, the need for intelligent automation to optimize CI/CD processes has become critical. This paper investigates the integration of AI-driven predictive models in DevOps pipelines, focusing on their ability to forecast build failures, optimize resource allocation, and streamline testing and deployment cycles. The study explores various AI techniques, including machine learning algorithms like regression, clustering, and neural networks, to address specific challenges in CI/CD processes. Predictive models trained on historical pipeline data can identify patterns, detect anomalies, and recommend proactive actions to prevent bottlenecks and failures. Additionally, the use of reinforcement learning enables dynamic resource management, ensuring efficient scaling during peak workloads. Key case studies illustrate the application of AI-driven predictive models in optimizing Jenkins and GitLab pipelines, achieving significant reductions in build times and improving deployment success rates. The research also highlights the role of AI in prioritizing test cases, automating performance monitoring, and enhancing feedback loops for continuous improvement. While emphasizing the benefits of AI integration, this paper also addresses challenges such as data quality, algorithm selection, and organizational readiness for adopting intelligent systems. By synthesizing these advancements, the paper provides a roadmap for leveraging AI to revolutionize DevOps workflows, paving the way for faster, more reliable software delivery in dynamic environments.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/JRCKUQSR/P3_OnePager.pdf;/Users/nilsarnold/Zotero/storage/YEZWS2C6/Enemosah - 2025 - Enhancing DevOps Efficiency through AI-Driven Predictive Models for Continuous Integration and Deplo.pdf}
}

@book{ghasemiIntroductionReinforcementLearning2024,
  title = {An {{Introduction}} to {{Reinforcement Learning}}: {{Fundamental Concepts}} and {{Practical Applications}}},
  shorttitle = {An {{Introduction}} to {{Reinforcement Learning}}},
  author = {Ghasemi, Majid and Moosavi, Amir and Sorkhoh, Ibrahim and Agrawal, Anjali and Alzhouri, Fadi and Ebrahimi, Dariush},
  year = 2024,
  month = aug,
  doi = {10.48550/arXiv.2408.07712},
  abstract = {Reinforcement Learning (RL) is a branch of Artificial Intelligence (AI) which focuses on training agents to make decisions by interacting with their environment to maximize cumulative rewards. An overview of RL is provided in this paper, which discusses its core concepts, methodologies, recent trends, and resources for learning. We provide a detailed explanation of key components of RL such as states, actions, policies, and reward signals so that the reader can build a foundational understanding. The paper also provides examples of various RL algorithms, including model-free and model-based methods. In addition, RL algorithms are introduced and resources for learning and implementing them are provided, such as books, courses, and online communities. This paper demystifies a comprehensive yet simple introduction for beginners by offering a structured and clear pathway for acquiring and implementing real-time techniques.},
  file = {/Users/nilsarnold/Zotero/storage/CSDBMLMV/Ghasemi et al. - 2024 - An Introduction to Reinforcement Learning Fundamental Concepts and Practical Applications.pdf}
}

@misc{GitOps2025OldSchool2025,
  title = {{{GitOps}} in 2025: {{From Old-School Updates}} to the {{Modern Way}}},
  shorttitle = {{{GitOps}} in 2025},
  year = 2025,
  month = jun,
  journal = {CNCF},
  urldate = {2025-11-30},
  abstract = {GitOps is now a foundational standard for managing modern applications, especially in Kubernetes environments. By the end of 2023, GitOps adoption surged, highlighting its role as a crucial pillar of software operations. It brings automation, consistency, and traceability to the otherwise chaotic world of cloud-native software.},
  howpublished = {https://www.cncf.io/blog/2025/06/09/gitops-in-2025-from-old-school-updates-to-the-modern-way/},
  langid = {american},
  file = {/Users/nilsarnold/Zotero/storage/6JZV9NIU/gitops-in-2025-from-old-school-updates-to-the-modern-way.html}
}

@book{govindarajanMachineLearningBased2025,
  title = {Machine {{Learning Based Approach}} for {{Handling Imbalanced Data}} for {{Intrusion Detection}} in the {{Cloud Environment}}},
  author = {Govindarajan, Vijay},
  year = 2025,
  month = mar,
  pages = {815},
  doi = {10.1109/ICDT63985.2025.10986614},
  file = {/Users/nilsarnold/Zotero/storage/G7JD83PL/Govindarajan - 2025 - Machine Learning Based Approach for Handling Imbalanced Data for Intrusion Detection in the Cloud En.pdf}
}

@article{guptaCloudNativeMLArchitecting2024a,
  title = {Cloud-{{Native ML}}: {{Architecting AI Solutions}} for {{Cloud-First Infrastructures}}},
  shorttitle = {Cloud-{{Native ML}}},
  author = {Gupta, Abhishek and Chaturvedi, Yashovardhan},
  year = 2024,
  month = dec,
  journal = {Nanotechnology Perceptions},
  volume = {20},
  pages = {930--939},
  doi = {10.62441/nano-ntp.v20i7.4004},
  abstract = {The integration of cloud-native architectures into artificial intelligence (AI) workflows has revolutionized the deployment, scalability, and efficiency of machine learning (ML) solutions. This study explores the design and evaluation of cloud-native ML models within cloud-first infrastructures, emphasizing their performance, cost-effectiveness, and scalability. Leveraging platforms such as AWS, Google Cloud, and Microsoft Azure, the research investigates data pipeline efficiency, model training metrics, inference performance, and economic viability. Statistical analyses reveal consistent accuracy, precision, and recall across models, with distinct trade-offs in resource utilization and latency between batch and real-time inference methods. The findings highlight the transformative potential of cloud-native ML in optimizing AI-driven decision-making, while identifying challenges such as resource allocation and cost management. This study serves as a foundation for advancing AI applications in cloud environments, offering insights for organizations to achieve greater agility and efficiency in AI deployment.},
  file = {/Users/nilsarnold/Zotero/storage/XIP7FXSY/Gupta und Chaturvedi - 2024 - Cloud-Native ML Architecting AI Solutions for Cloud-First Infrastructures.pdf}
}

@misc{huMLOpsMonitoringScale2025,
  title = {{{MLOps Monitoring}} at {{Scale}} for {{Digital Platforms}}},
  author = {Hu, Yu Jeffrey and Rombouts, Jeroen and Wilms, Ines},
  year = 2025,
  month = apr,
  number = {arXiv:2504.16789},
  eprint = {2504.16789},
  primaryclass = {econ},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16789},
  urldate = {2025-11-18},
  abstract = {Machine learning models are widely recognized for their strong performance in forecasting. To keep that performance in streaming data settings, they have to be monitored and frequently re-trained. This can be done with machine learning operations (MLOps) techniques under supervision of an MLOps engineer. However, in digital platform settings where the number of data streams is typically large and unstable, standard monitoring becomes either suboptimal or too labor intensive for the MLOps engineer. As a consequence, companies often fall back on very simple worse performing ML models without monitoring. We solve this problem by adopting a design science approach and introducing a new monitoring framework, the Machine Learning Monitoring Agent (MLMA), that is designed to work at scale for any ML model with reasonable labor cost. A key feature of our framework concerns test-based automated re-training based on a data-adaptive reference loss batch. The MLOps engineer is kept in the loop via key metrics and also acts, pro-actively or retrospectively, to maintain performance of the ML model in the production stage. We conduct a large-scale test at a last-mile delivery platform to empirically validate our monitoring framework.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Applications},
  file = {/Users/nilsarnold/Zotero/storage/Y7BUH58I/Hu et al. - 2025 - MLOps Monitoring at Scale for Digital Platforms.pdf;/Users/nilsarnold/Zotero/storage/CHRTNYLE/2504.html}
}

@article{jossonpaulkalapparambathAdvancingDistributedSystems2025,
  title = {Advancing {{Distributed Systems}} with {{Reinforcement Learning}}: {{A New Frontier}} in {{AI-Integrated Software Engineering}}},
  shorttitle = {Advancing {{Distributed Systems}} with {{Reinforcement Learning}}},
  author = {Josson Paul Kalapparambath and Yugandhar Suthari and Gaurav Mishra},
  year = 2025,
  month = mar,
  publisher = {SARC Publisher},
  issn = {2945-3585},
  doi = {10.5281/ZENODO.15305618},
  urldate = {2025-12-02},
  abstract = {The integration of reinforcement learning (RL) into distributed systems represents a transformative advancement in AIintegrated software engineering. This study explores the potential of RL algorithms, including Q-learning, DQN, and PPO, to optimize key aspects of distributed systems such as resource allocation, load balancing, fault tolerance, and system efficiency. Through extensive experimentation and statistical analysis, we demonstrate that RL-driven approaches significantly outperform traditional methods, achieving improvements of up to 25\% in resource utilization, 30\% in load distribution fairness, and 40\% in fault recovery time. The robustness of RL models is further validated under extreme conditions, with only a 10\% degradation in system efficiency compared to 30\% for baseline methods. Additionally, the integration of RL with AI-driven software engineering practices, such as modular code, CI/CD pipelines, and automated testing, reduces development time by 20\% and error rates by 15\%. These findings highlight the adaptability, scalability, and resilience of RL-integrated distributed systems, making them well-suited for dynamic and complex environments. Despite challenges such as computational costs and model interpretability, this research underscores the transformative potential of RL in advancing distributed systems and shaping the future of intelligent, self-optimizing software architectures.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/VWLCKH4W/Josson Paul Kalapparambath et al. - 2025 - Advancing Distributed Systems with Reinforcement Learning A New Frontier in AI-Integrated Software.pdf}
}

@misc{juliakmWhatPlatformEngineering,
  title = {What Is {{Platform Engineering}}?},
  author = {{juliakm}},
  urldate = {2025-12-15},
  abstract = {Learn about how platform engineering improves each development team's security, compliance, costs, and time-to-business value.},
  howpublished = {https://learn.microsoft.com/en-us/platform-engineering/what-is-platform-engineering},
  langid = {american},
  file = {/Users/nilsarnold/Zotero/storage/EXLSEVGH/what-is-platform-engineering.html}
}

@article{kankanalaAIMLDevOps2024,
  title = {{{AI}}/{{ML}} -- {{DevOps Automation}}},
  author = {Kankanala, Giridhar and Amgothu, Sudheer},
  year = 2024,
  month = oct,
  volume = {13},
  pages = {111--117},
  abstract = {The rapid advancement of Artificial Intelligence (AI) and Machine Learning (ML) is significantly transforming DevOps, particularly in cloud environments, where the need for speed, security, and scalability is paramount. This research investigates how AI/ML enhances automation within DevOps workflows, improves threat detection capabilities, and optimizes workload distribution in cloud-native environments. By integrating AI/ML into Continuous Integration/Continuous Deployment (CI/CD) pipelines, we aim to improve operational efficiency and detect threats with greater precision. We also evaluate the performance of AI-based workload optimization in dynamically scaling resources, reducing latency, and minimizing resource wastage. Our experimental results demonstrate that AI-powered systems can reduce deployment times by up to 30\%, improve threat detection rates by 45\%, and significantly enhance resource management compared to traditional systems.},
  file = {/Users/nilsarnold/Zotero/storage/9WYXFTMW/Kankanala und Amgothu - 2024 - AIML â€“ DevOps Automation.pdf}
}

@article{karthikputhrayaRoleCloudNativeArchitectures2025,
  title = {The {{Role}} of {{Cloud-Native Architectures}} in {{Accelerating Machine Learning Workflows}} through {{Data Engineering Innovations}}},
  author = {Karthik Puthraya and Rachit Gupta and Beverly DSouza},
  year = 2025,
  month = feb,
  publisher = {SARC Publisher},
  issn = {2945-3437},
  doi = {10.5281/ZENODO.15106432},
  urldate = {2025-11-05},
  abstract = {The rapid advancement of machine learning (ML) necessitates scalable, efficient, and cost-effective computing environments. Traditional ML workflows often face challenges related to long training times, high inference latency, infrastructure costs, and scalability limitations. This study explores the role of cloud-native architectures in accelerating ML workflows through data engineering innovations. By leveraging microservices, containerization, serverless computing, and automated data pipelines, cloud-native environments optimize ML operations while reducing computational overhead. The results indicate a 50\% reduction in training time and inference latency, 50-55\% cost savings, and a threefold increase in scalability compared to traditional ML implementations. Moreover, cloud-native solutions enhance fault tolerance by reducing system recovery time by 80\%, ensuring greater reliability for real-time AI applications. Statistical analyses, including regression modeling, survival analysis, and PCA, confirm the efficiency gains of cloud-based ML workflows. The findings suggest that organizations adopting cloud-native ML architectures can achieve faster model deployment, reduced infrastructure costs, and enhanced system resilience. As cloud-native computing evolves, its integration with machine learning will play a pivotal role in shaping future AI-driven solutions.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/2GBV73NT/Karthik Puthraya et al. - 2025 - The Role of Cloud-Native Architectures in Accelerating Machine Learning Workflows through Data Engin.pdf}
}

@article{kathiresanCybersecurityRiskModeling2025,
  title = {Cybersecurity {{Risk Modeling}} in {{CI}}/{{CD Pipelines Using Reinforcement Learning}} for {{Test Optimization}}},
  author = {Kathiresan, Gopinath},
  year = 2025,
  month = may,
  journal = {International Journal of Innovative Science and Research Technology},
  pages = {15--25},
  doi = {10.38124/ijisrt/25may339},
  urldate = {2025-11-03},
  abstract = {Incremental software development and deployment brought about the much-advertised Continuous Integration and Continuous Deployment (CI/CD) approaches that have changed completely how modern applications are constructed, tested, and launched. But the fast-delivery strategy hugely opened the gates to cyber threats, giving CI/CD pipelines the status of most-sought cyber-hacking targets. Traditional static security models have been frequently experienced to fail in in line with the dynamic nature of CI/CD workflows, hence allowing undetected vulnerabilities to persist and prolonging remediation. This study proposes the utilization of reinforcement learning (RL) for optimizing cybersecurity risk modeling and testing in CI/CD pipelines. The system makes maximum use of real-time threat intelligence, in combination with dynamic test selection techniques, toward maximum detection of vulnerabilities within the smallest possible amount of resource allocation. RL agents are trained to always push severe test scenarios first in a way to better absorb changing attacks and codebase dynamics. Empirical study results show improved detection rates, less test time, and better risk visibility in all stages of the pipeline, marking a major fight toward intelligent and adaptive DevOps security practices.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/3BR9GY3W/Kathiresan - 2025 - Cybersecurity Risk Modeling in CICD Pipelines Using Reinforcement Learning for Test Optimization.pdf}
}

@misc{kitchenhamPDFGuidelinesPerforming,
  title = {({{PDF}}) {{Guidelines}} for Performing {{Systematic Literature Reviews}} in {{Software Engineering}}},
  author = {Kitchenham, Barbara and Charters, Stuart M.},
  journal = {ResearchGate},
  urldate = {2025-11-06},
  abstract = {PDF \textbar{} The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering... \textbar{} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/302924724\_Guidelines\_for\_performing\_Systematic\_Literature\_Reviews\_in\_Software\_Engineering},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/BYTHTHSA/(PDF) Guidelines for performing Systematic Literature Reviews in Software Engineering.pdf;/Users/nilsarnold/Zotero/storage/YVQEJZER/Kitchenham-2007Systematicreviews5-8.pdf}
}

@misc{luComputingEraLarge2024a,
  title = {Computing in the {{Era}} of {{Large Generative Models}}: {{From Cloud-Native}} to {{AI-Native}}},
  shorttitle = {Computing in the {{Era}} of {{Large Generative Models}}},
  author = {Lu, Yao and Bian, Song and Chen, Lequn and He, Yongjun and Hui, Yulong and Lentz, Matthew and Li, Beibin and Liu, Fei and Li, Jialin and Liu, Qi and Liu, Rui and Liu, Xiaoxuan and Ma, Lin and Rong, Kexin and Wang, Jianguo and Wu, Yingjun and Wu, Yongji and Zhang, Huanchen and Zhang, Minjia and Zhang, Qizhen and Zhou, Tianyi and Zhuo, Danyang},
  year = 2024,
  month = jan,
  number = {arXiv:2401.12230},
  eprint = {2401.12230},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.12230},
  urldate = {2025-11-12},
  abstract = {In this paper, we investigate the intersection of large generative AI models and cloud-native computing architectures. Recent large models such as ChatGPT, while revolutionary in their capabilities, face challenges like escalating costs and demand for high-end GPUs. Drawing analogies between large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we describe an AI-native computing paradigm that harnesses the power of both cloud-native technologies (e.g., multi-tenancy and serverless computing) and advanced machine learning runtime (e.g., batched LoRA inference). These joint efforts aim to optimize costs-of-goods-sold (COGS) and improve resource accessibility. The journey of merging these two domains is just at the beginning and we hope to stimulate future research and development in this area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {/Users/nilsarnold/Zotero/storage/T66PHMMC/Lu et al. - 2024 - Computing in the Era of Large Generative Models From Cloud-Native to AI-Native.pdf;/Users/nilsarnold/Zotero/storage/U4TXCUXT/2401.html}
}

@misc{Overview,
  title = {Overview},
  journal = {Kubernetes},
  urldate = {2025-11-15},
  abstract = {Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services that facilitate both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.},
  howpublished = {https://kubernetes.io/docs/concepts/overview/},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/XKRBZBKI/overview.html}
}

@book{pandaScalableArtificialIntelligence2025a,
  title = {Scalable {{Artificial Intelligence Systems}}: {{Cloud-Native}}, {{Edge-AI}}, {{MLOps}}, and {{Governance}} for {{Real-World Deployment}}},
  shorttitle = {Scalable {{Artificial Intelligence Systems}}},
  author = {Panda, Swarup},
  year = 2025,
  month = jul,
  publisher = {Deep Science Publishing},
  abstract = {Artificial Intelligence (AI) has become essential across industries, transforming operations, decision-making, and value creation. As organizations worldwide use AI to address challenges in areas like healthcare, finance, cybersecurity, manufacturing, and infrastructure, the need for reliable and scalable AI systems continues to grow.~This book offers practical guidance for professionals designing and deploying scalable, compliant AI solutions in production environments. It covers modernizing legacy systems, building MLOps pipelines, and addressing ethical aspects of autonomous AI, providing essential insights and patterns for real-world applications.~We cover essential topics for enterprise AI success, such as scalable architectures (cloud-native, edge, hybrid), MLOps for lifecycle management, and governance for compliance and fairness. The text also outlines frameworks for explainable and federated AI in regulated fields, supporting privacy and distributed intelligence.~We demonstrate AI\&\#39;s impact on diagnostics, fraud detection, threat intelligence, and urban planning through case studies, and review how platforms like Azure, AWS, and GCP support scalable AI deployment.~This book highlights the need for ethical AI that upholds human values, privacy, and transparency. As AI shapes society, we must design, deploy, and govern it responsibly.~I invite you to explore these chapters with a mindset of both innovation and accountability---as together, we shape a future powered by intelligent and responsible systems.},
  googlebooks = {hI52EQAAQBAJ},
  isbn = {978-93-7185-753-6},
  langid = {english},
  keywords = {Computers / Data Science / Machine Learning,Computers / Distributed Systems / Cloud Computing,Computers / Quantum Computing}
}

@article{petersenGuidelinesConductingSystematic2015,
  title = {Guidelines for Conducting Systematic Mapping Studies in Software Engineering: {{An}} Update},
  shorttitle = {Guidelines for Conducting Systematic Mapping Studies in Software Engineering},
  author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
  year = 2015,
  month = aug,
  journal = {Information and Software Technology},
  volume = {64},
  pages = {1--18},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2015.03.007},
  urldate = {2025-11-06},
  abstract = {Context Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.},
  keywords = {Guidelines,Software engineering,Systematic mapping studies}
}

@article{poudelAIDrivenIntelligentAutoScaling2025a,
  title = {{{AI-Driven Intelligent Auto-Scaling}} for {{Cloud Resource Optimization}} 1},
  author = {Poudel, Sudip and Marasini, Kushal and Bhatt, Laxmi and Baral, Daya},
  year = 2025,
  month = oct,
  journal = {Journal of Advanced College of Engineering and Management},
  volume = {Vol. 11},
  pages = {27--36},
  doi = {10.3126/jacem.v11i1.84521},
  abstract = {This study introduces a predictive, AI-powered auto-scaling framework designed to optimize resource usage in cloud environments, specifically within Amazon Web Services (AWS). Conventional rule-based scaling methods often result in inefficiencies, either wasting resources or degrading performance. To overcome these challenges, this work employs Long Short-Term Memory (LSTM) neural networks that analyze historical performance data collected from AWS CloudWatch. The system forecasts resource demand trends for EC2 and RDS instances and automates scaling actions using the Boto3 SDK. It evaluates multiple metrics-including CPU usage, memory availability, disk I/O, and network traffic-to make accurate, real-time decisions. Operating in a continuous loop, the model updates hourly to adapt to changing workloads. Experimental evaluation confirms that the proposed approach reduces operational costs and enhances performance reliability. This research delivers a scalable, intelligent solution for cloud resource management, suitable for dynamic application environments where responsiveness and efficiency are critical.},
  file = {/Users/nilsarnold/Zotero/storage/UYY9B9UJ/Poudel et al. - 2025 - AI-Driven Intelligent Auto-Scaling for Cloud Resource Optimization 1.pdf}
}

@article{reddygopireddyIntegratingAIDevOps2022,
  title = {Integrating {{AI}} into {{DevOps}}: {{Leveraging Machine Learning}} for {{Intelligent Automation}} in {{Azure}}},
  shorttitle = {Integrating {{AI}} into {{DevOps}}},
  author = {Reddy Gopireddy, Satheesh},
  year = 2022,
  month = jun,
  journal = {International Journal of Science and Research (IJSR)},
  volume = {11},
  number = {6},
  pages = {2035--2039},
  issn = {23197064},
  doi = {10.21275/SR22619111757},
  urldate = {2025-11-03},
  abstract = {The convergence of Artificial Intelligence (AI) and DevOps is paving the way for transformative changes in the way software is developed, deployed, and managed, particularly in cloud environments like Microsoft Azure. By integrating AI, specifically Machine Learning (ML), into DevOps practices, organizations can achieve intelligent automation, predictive analytics, anomaly detection, and continuous optimization. This paper explores how AI can be effectively leveraged within Azure's DevOps ecosystem, offering insights into the benefits, challenges, and best practices associated with this integration. The article delves into case studies, provides strategic frameworks, and discusses future trends that will shape the evolution of AI-driven DevOps.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/7Q949NIR/Reddy Gopireddy - 2022 - Integrating AI into DevOps Leveraging Machine Learning for Intelligent Automation in Azure.pdf}
}

@misc{rzigEmpiricalAnalysisCI2024,
  title = {Empirical {{Analysis}} on {{CI}}/{{CD Pipeline Evolution}} in {{Machine Learning Projects}}},
  author = {Rzig, Dhia Elhaq and Houerbi, Alaa and Chavan, Rahul Ghanshyam and Hassan, Foyzul},
  year = 2024,
  month = mar,
  journal = {arXiv.org},
  urldate = {2025-12-16},
  abstract = {The growing popularity of machine learning (ML) and the integration of ML components with other software artifacts has led to the use of continuous integration and delivery (CI/CD) tools, such as Travis CI, GitHub Actions, etc. that enable faster integration and testing for ML projects. Such CI/CD configurations and services require synchronization during the life cycle of the projects. Several works discussed how CI/CD configuration and services change during their usage in traditional software systems. However, there is very limited knowledge of how CI/CD configuration and services change in ML projects. To fill this knowledge gap, this work presents the first empirical analysis of how CI/CD configuration evolves for ML software systems. We manually analyzed 343 commits collected from 508 open-source ML projects to identify common CI/CD configuration change categories in ML projects and devised a taxonomy of 14 co-changes in CI/CD and ML components. Moreover, we developed a CI/CD configuration change clustering tool that identified frequent CI/CD configuration change patterns in 15,634 commits. Furthermore, we measured the expertise of ML developers who modify CI/CD configurations. Based on this analysis, we found that 61.8\% of commits include a change to the build policy and minimal changes related to performance and maintainability compared to general open-source projects. Additionally, the co-evolution analysis identified that CI/CD configurations, in many cases, changed unnecessarily due to bad practices such as the direct inclusion of dependencies and a lack of usage of standardized testing frameworks. More practices were found through the change patterns analysis consisting of using deprecated settings and reliance on a generic build language. Finally, our developer's expertise analysis suggests that experienced developers are more inclined to modify CI/CD configurations.},
  howpublished = {https://arxiv.org/abs/2403.12199v4},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/JEZFVN7I/Rzig et al. - 2024 - Empirical Analysis on CICD Pipeline Evolution in Machine Learning Projects.pdf}
}

@article{sikhaCloudNativeApplicationDevelopment2023,
  title = {Cloud-{{Native Application Development}} for {{AI- Conducive Architectures}}.},
  author = {Sikha, Vijay Kartik},
  year = 2023,
  journal = {International Journal on Recent and Innovation Trends in Computing and Communication},
  volume = {11},
  number = {11},
  abstract = {The integration of AI technologies into cloud-native application development is reshaping the landscape of software engineering by enhancing scalability, flexibility, and efficiency. This paper explores the evolution of development processes driven by advancements in AI and cloud technologies, emphasizing the importance of adopting an AI mindset. It discusses how cloud-native architectures, characterized by microservices and containerization, align well with AI requirements, enabling organizations to deploy and scale AI-driven applications effectively. Key guidelines for balancing innovation with cost-efficiency are outlined, including leveraging generative AI tools, optimizing resource management, and implementing observability practices. The paper also examines the impact of AI on architectural decision-making, such as data management and real-time processing, and highlights the role of databases in managing extensive data required for AI tasks. Additionally, the paper compares legacy tools with AI-driven tools in accelerating development processes and addresses security considerations in AI-enabled cloud-native applications. The future direction of AI-driven cloud-native development is discussed, focusing on advancements in efficiency, scalability, and security.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/3F5K73RF/Sikha - 2023 - Cloud-Native Application Development for AI- Conducive Architectures..pdf}
}

@article{supritpattanayakIntegratingAIDevOps2024,
  title = {Integrating {{AI}} into {{DevOps}} Pipelines: {{Continuous}} Integration, Continuous Delivery, and Automation in Infrastructural Management: {{Projections}} for Future},
  shorttitle = {Integrating {{AI}} into {{DevOps}} Pipelines},
  author = {{Suprit Pattanayak} and {Pranav Murthy} and {Aditya Mehra}},
  year = 2024,
  month = oct,
  journal = {International Journal of Science and Research Archive},
  volume = {13},
  number = {1},
  pages = {2244--2256},
  issn = {25828185},
  doi = {10.30574/ijsra.2024.13.1.1838},
  urldate = {2025-11-03},
  abstract = {This paper explores the transformative role of Artificial Intelligence (AI) in enhancing DevOps pipelines, focusing on Continuous Integration (CI), Continuous Delivery (CD), and automation in infrastructural management. As organizations increasingly adopt DevOps methodologies to streamline software development and delivery processes, the integration of AI technologies offers significant potential for improving efficiency, quality, and responsiveness. This study reviews current practices in CI/CD and examines how AI-driven tools can automate repetitive tasks, optimize resource allocation, and facilitate predictive analytics for proactive decision-making. Additionally, the paper discusses the challenges and considerations associated with AI integration in DevOps, including cultural shifts, data governance, and the need for skilled personnel. Projections for future developments highlight the potential for AI to create smarter, more adaptive DevOps environments that align with evolving industry demands. By identifying key trends and innovations, this research provides a comprehensive overview of the future landscape of DevOps, positioning AI as a critical enabler of agility and performance.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/8T3VMWHP/Suprit Pattanayak et al. - 2024 - Integrating AI into DevOps pipelines Continuous integration, continuous delivery, and automation in.pdf}
}

@article{tamanampudiAIEnhancedContinuousIntegration,
  title = {{{AI-Enhanced Continuous Integration}} and {{Continuous Deployment Pipelines}}: {{Leveraging Machine Learning Models}} for {{Predictive Failure Detection}}, {{Automated Rollbacks}}, and {{Adaptive Deployment Strategies}} in {{Agile Software Development}}},
  author = {Tamanampudi, Venkata Mohit},
  volume = {10},
  abstract = {The integration of artificial intelligence (AI) and machine learning (ML) into Continuous Integration and Continuous Deployment (CI/CD) pipelines has the potential to significantly enhance the agility, reliability, and efficiency of software development processes. This research paper investigates the application of AI-enhanced methodologies within CI/CD pipelines, focusing on how machine learning models can be utilized to address core challenges in agile software development, particularly in the domains of predictive failure detection, automated rollbacks, and adaptive deployment strategies. The study posits that by embedding intelligent systems into CI/CD workflows, software teams can mitigate risks, reduce downtime, and achieve more reliable and faster releases, while simultaneously improving overall software quality.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/6CG7I9M7/Tamanampudi - AI-Enhanced Continuous Integration and Continuous Deployment Pipelines Leveraging Machine Learning.pdf}
}

@article{tamminediAutomatingKubernetesOperations2024,
  title = {Automating {{Kubernetes Operations}} with {{AI}} and {{Machine Learning}}},
  author = {Tamminedi, Varun},
  year = 2024,
  month = dec,
  journal = {IJFMR - International Journal For Multidisciplinary Research},
  volume = {6},
  number = {6},
  publisher = {IJFMR},
  issn = {2582-2160},
  doi = {10.36948/ijfmr.2024.v06i06.33430},
  urldate = {2025-11-03},
  copyright = {Creative Commons Attribution-ShareAlike 4.0 International License},
  file = {/Users/nilsarnold/Zotero/storage/95KDISVQ/Tamminedi - 2024 - Automating Kubernetes Operations with AI and Machine Learning.pdf}
}

@book{tatineniIntegratingArtificialIntelligence2024b,
  title = {Integrating {{Artificial Intelligence}} with {{DevOps}}: {{Advanced Techniques}}, {{Predictive Analytics}}, and {{Automation}} for {{Real-Time Optimization}} and {{Security}} in {{Modern Software Development}}},
  shorttitle = {Integrating {{Artificial Intelligence}} with {{DevOps}}},
  author = {Tatineni, Sumanth},
  year = 2024,
  month = mar,
  publisher = {Libertatem Media Private Limited},
  abstract = {Unlock the future of software development with Integrating Artificial Intelligence with DevOps: Advanced Techniques, Predictive Analytics, and Automation for Real-Time Optimization and Security in Modern Software Development. This comprehensive monograph is a must-read for professionals seeking to revolutionize their DevOps workflows through the power of AI.Dive deep into the intricate integration of Artificial Intelligence within DevOps practices and discover advanced methodologies that enhance every stage of the software development lifecycle. From predictive analytics and intelligent automation to real-time optimization and robust security measures, this book offers a wealth of knowledge for optimizing software delivery.Explore practical applications, in-depth case studies, and best practices that illustrate the transformative potential of AI in DevOps. Each chapter builds on the previous, providing a seamless and cohesive narrative that guides readers through foundational concepts to advanced implementations. Whether you\&\#39;re looking to improve CI/CD pipelines, automate testing and monitoring, manage infrastructure more efficiently, or enhance security with AI-driven techniques, this book equips you with the tools and insights needed to ensure high-quality, secure, and efficient software delivery.Join the vanguard of modern software development with Integrating Artificial Intelligence with DevOps, and harness AI to achieve real-time optimization and unparalleled security in your DevOps processes.},
  googlebooks = {86YfEQAAQBAJ},
  isbn = {978-81-971382-1-8},
  langid = {english},
  keywords = {Computers / Information Technology}
}

@article{uddohAIBasedThreatDetection2021,
  title = {{{AI-Based Threat Detection Systems}} for {{Cloud Infrastructure}}: {{Architecture}}, {{Challenges}}, and {{Opportunities}}},
  shorttitle = {{{AI-Based Threat Detection Systems}} for {{Cloud Infrastructure}}},
  author = {Uddoh, Jeanette and Ajiga, Daniel and Okare, Babawale Patrick and Aduloju, Tope David},
  year = 2021,
  journal = {Journal of Frontiers in Multidisciplinary Research},
  volume = {2},
  number = {2},
  pages = {61--67},
  issn = {30509718, 30509726},
  doi = {10.54660/.IJFMR.2021.2.2.61-67},
  urldate = {2025-11-04},
  abstract = {The rapid adoption of cloud infrastructure has transformed organizational operations, offered scalability and flexibility but also exposed enterprises to sophisticated cyber threats, such as data breaches, ransomware, and insider attacks. AI-based threat detection systems have emerged as a critical solution, leveraging machine learning, deep learning, and behavioral analytics to identify and mitigate threats in real time. This paper proposes novel architecture for AI-based threat detection in cloud infrastructure, addressing the unique challenges of dynamic, distributed environments. Through a systematic literature review and mixed-method evaluation, the study synthesizes insights from cybersecurity, cloud computing, and AI research, drawing on 100 peer-reviewed articles and industry reports from 2015 to 2025. The proposed architecture integrates real-time data ingestion, anomaly detection, threat classification, and automated response, optimized for scalability and resilience. Key findings reveal that architecture achieves 95\% accuracy in detecting advanced threats, reducing false positives by 20\% compared to traditional systems. However, challenges such as computational complexity, data privacy, and integration with legacy systems pose significant hurdles. Opportunities include leveraging federated learning and quantum computing to enhance detection capabilities. The study contributes to cybersecurity literature by offering a scalable, AI-driven architecture that balances performance and practicality, with implications for cloud providers, enterprises, and policymakers. For practitioners, architecture provides a blueprint for securing cloud environments, while researchers can explore future directions, such as AI explainability and zero-trust integration. By addressing architecture design, challenges, and opportunities, this paper underscores the transformative potential of AI-based threat detection in safeguarding cloud infrastructure, fostering resilience, and enabling secure digital transformation in an increasingly threat-prone landscape.},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/CKZTISVG/Uddoh et al. - 2021 - AI-Based Threat Detection Systems for Cloud Infrastructure Architecture, Challenges, and Opportunit.pdf}
}

@article{valkenborgSupervisedLearning2023,
  title = {Supervised Learning},
  author = {Valkenborg, Dirk and Geubbelmans, Melvin and Rousseau, Axel-Jan and Burzykowski, Tomasz},
  year = 2023,
  month = jul,
  journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
  volume = {164},
  number = {1},
  pages = {146--149},
  issn = {08895406},
  doi = {10.1016/j.ajodo.2023.04.010},
  urldate = {2025-11-27},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/JVXGPF3Z/Valkenborg et al. - 2023 - Supervised learning.pdf}
}

@article{valkenborgUnsupervisedLearning2023,
  title = {Unsupervised Learning},
  author = {Valkenborg, Dirk and Rousseau, Axel-Jan and Geubbelmans, Melvin and Burzykowski, Tomasz},
  year = 2023,
  month = jun,
  journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
  volume = {163},
  number = {6},
  pages = {877--882},
  issn = {08895406},
  doi = {10.1016/j.ajodo.2023.04.001},
  urldate = {2025-11-27},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/N5EH2373/Valkenborg et al. - 2023 - Unsupervised learning.pdf}
}

@misc{WasIstCI,
  title = {{Was ist CI/CD? \textbar{} Automatisierung in der Softwareentwicklung}},
  shorttitle = {{Was ist CI/CD?}},
  urldate = {2025-12-16},
  abstract = {Erfahren Sie, wie Continuous Integration \& Continuous Deployment funktionieren, was eine CI/CD-Pipeline ist und ihre Bedeutung in der Softwareentwicklung.},
  howpublished = {https://www.redhat.com/de/topics/devops/what-is-ci-cd},
  langid = {ngerman},
  file = {/Users/nilsarnold/Zotero/storage/CVR4D9ZF/what-is-ci-cd.html}
}

@misc{WasIstDevOps,
  title = {{Was ist DevOps? Erl\"auterung zu DevOps \textbar{} Microsoft Azure}},
  shorttitle = {{Was ist DevOps?}},
  urldate = {2025-12-16},
  abstract = {Erfahren Sie, was DevOps ist und wie DevOps-Verfahren und -Rollen die Automatisierung und Zusammenarbeit verbessern, um bessere Produkte f\"ur Kunden zu entwickeln.},
  howpublished = {https://azure.microsoft.com/de-de/resources/cloud-computing-dictionary/what-is-devops},
  langid = {german},
  file = {/Users/nilsarnold/Zotero/storage/ZLGJRHST/what-is-devops.html}
}

@inproceedings{wohlinGuidelinesSnowballingSystematic2014,
  title = {Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Evaluation}} and {{Assessment}} in {{Software Engineering}}},
  author = {Wohlin, Claes},
  year = 2014,
  month = may,
  series = {{{EASE}} '14},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2601248.2601268},
  urldate = {2025-11-06},
  abstract = {Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.},
  isbn = {978-1-4503-2476-2}
}

@article{zaaloukCLOUDNATIVEARTIFICIAL,
  title = {{{CLOUD NATIVE ARTIFICIAL INTELLIGENCE}}},
  author = {Zaalouk, Adel and Jones, Alex and Velichkevich, Andrey and Kurktchiev, Boris and Chin, Cassandra and Zhang, Cathy and Misale, Claudia and Chen, Huamin and Roberts, Joel and Chen, Kai-Hsun and Bhandaru, Malini and Yao, Michael and Raghunath, Nikhita and Pan, Peter and Kakodkar, Rajas and Pandey, Rasik and Aravena, Ricardo and Petty, Ronald and Taylor, Ryan and Sheikh, Saad and Wilson, Shawn and Thorley, Tom and Lu, Victor},
  langid = {english},
  file = {/Users/nilsarnold/Zotero/storage/JTPD487Y/Zaalouk et al. - NATIVE ARTIFICIAL INTELLIGENCE.pdf}
}
