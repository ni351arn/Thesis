\section{Beantwortung der Forschungsfragen}
\label{sec:beantwortung-ff}
In diesem Abschnitt werden die drei Forschungsfragen der Arbeit (RQ1, RQ2, RQ3) auf Basis der erarbeiteten Ergebnisse beantwortet.
Die Antworten beziehen sich auf die in der Literaturanalyse identifizierten Anwendungsfelder und Herausforderungen (RQ1), die untersuchten \gls{ki}-Methoden und -Werkzeuge (RQ2) sowie das entwickelte Bewertungskonzept und dessen Anwendung auf einen konkreten Anwendungsfall (RQ3).
Damit werden die in \autoref{sec:forschungsfragen} formulierten Fragen beantwortet und die Erkenntnisse in den Gesamtkontext des Cloud-Native Platform Engineerings eingeordnet.

\subsection{Beantwortung der Forschungsfrage RQ1}
\label{rq1Antwort}
Die Literaturanalyse zeigt vier wiederkehrende Anwendungsfelder, in denen \gls{ki} einen Mehrwert bringt und gleichzeitig Herausforderungen auftreten.
Diese vier Felder lassen sich als konkrete betriebliche Aufgaben einordnen: Ressourcen- und Workload-Optimierung, Betrieb und Orchestrierung der Plattform, Optimierung von \gls{ci}/\gls{cd}-Pipelines sowie Sicherheits- und Bedrohungserkennung. 

Ressourcen- und Workload-Optimierung umfasst die Skalierung von Workloads, zum Beispiel die Anpassung der Anzahl von Kubernetes-Pods bei schwankender Last. 
\gls{ki} wertet Auslastungs- und Latenzverläufe aus und sagt kurzfristige Lastphasen voraus, sodass Skalierungsentscheidungen früher getroffen werden. 
Dadurch sinkt das Risiko verspäteter Skalierung und eine Überbereitstellung wird reduziert. 
Grenzen entstehen, wenn historische Daten fehlen oder Lastmuster sich häufig ändern \cite{tamminediAutomatingKubernetesOperations2024,poudelAIDrivenIntelligentAutoScaling2025a}.

Betrieb und Orchestrierung adressiert Störungen im laufenden Betrieb sowie deren Eingrenzung über mehrere Komponenten hinweg. 
Auf Basis von Protokollen, Metriken und Ereignissen werden Ursachen wahrscheinlicher Komponenten abgeleitet. 
Zusätzlich werden Maßnahmen wie Neustarts, Umplanungen oder Konfigurationsanpassungen priorisiert und koordiniert angestoßen.
Unzuverlässig wird der Ansatz, wenn sich das Normalverhalten durch Releases und Konfigurationsänderungen häufig verschiebt und dadurch Fehlalarme entstehen \cite{tamminediAutomatingKubernetesOperations2024,tamanampudiAIEnhancedContinuousIntegration}.

Die Optimierung von \gls{ci}/\gls{cd}-Pipelines zielt darauf ab, Fehler früher zu erkennen und Auslieferungen robuster zu machen.
Während schrittweiser Rollouts werden Fehlerraten und Antwortzeiten überwacht.
Bei Abweichungen vom erwarteten Normalzustand wird die Auslieferung gestoppt oder auf eine stabile Version zurückgesetzt.
Problematisch wird dies, wenn normale Schwankungen nicht zuverlässig von echten Fehlern getrennt werden und dadurch unnötige Rücksetzungen ausgelöst werden \cite{tamanampudiAIEnhancedContinuousIntegration}.

Sicherheits- und Bedrohungserkennung umfasst Anomalieerkennung, Echtzeiterkennung von Angriffen sowie die Klassifikation von Bedrohungen. 
Ein Beispiel ist der Einsatz von \gls{sl}-Verfahren, um erkannte Anomalien als bösartig oder gutartig einzuordnen. 
Grenzen entstehen durch sensible Daten und Vorgaben, weil Protokolle und Sicherheitsdaten nicht beliebig ausgewertet und aufbewahrt werden dürfen \cite{kankanalaAIMLDevOps2024,uddohAIBasedThreatDetection2021}.

Über alle Anwendungsfelder zeigen sich wiederkehrende Herausforderungen.
Dazu zählen zusätzlicher Rechen- und Betriebsaufwand, Integrationsaufwand sowie Einschränkungen durch Datenschutz und Freigaben.
Am häufigsten limitiert jedoch die Datenbasis, weil unvollständige oder uneinheitliche Daten zu instabilen Ergebnissen führen.

\subsection{Beantwortung der Forschungsfrage RQ2}
\label{rq2Antwort}
Die Literatur zeigt kein einheitliches Lernparadigma, Werkzeug oder Algorithmus, der für alle Anwendungsfälle gleichermaßen geeignet ist.
Die Wahl hängt vom Anwendungsfall, von der verfügbaren Datenbasis und von der Systemarchitektur der Plattform ab.

Am häufigsten werden \gls{sl}-Verfahren thematisiert und eingesetzt.
Sie eignen sich, wo historische Daten mit bekannten Ergebnissen vorliegen, zum Beispiel erfolgreiche und fehlgeschlagene Builds oder klassifizierte Störungen.
Damit lassen sich Vorhersagen für Lastverläufe erstellen oder Fehlertypen aus Protokollen einordnen.
Auch die Bewertung von Änderungsrisiken in \gls{ci}/\gls{cd}-Pipelines wird so umgesetzt, indem frühere Pipeline-Verläufe als Trainingsgrundlage dienen \cite{enemosahEnhancingDevOpsEfficiency2025}.

\gls{ul}-Verfahren werden dort eingesetzt, wo solche Zuordnungen fehlen.
Typisch ist die Erkennung von Abweichungen in Metriken oder Protokollen, ohne dass vorher markiert wurde, was ein Fehler ist.
Das ist im Betrieb hilfreich, erfordert aber saubere Referenzwerte und robuste Schwellen. 
Ohne diese sinkt die Trefferqualität und es entstehen mehr Fehlalarme \cite{tamanampudiAIEnhancedContinuousIntegration}.

Etwas weniger thematisiert aber trotzdem relevant sind \gls{rl}-Ansätze.
Diese werden vor allem für Steuerungsentscheidungen beschrieben, zum Beispiel für Skalierung oder Rollout-Entscheidungen unter wechselnden Bedingungen.
Der Aufwand ist hoch, weil Training und Rückkopplung sauber abgebildet werden müssen.
Daher ist der Einsatz in produktiven Umgebungen bislang begrenzt \cite{jossonpaulkalapparambathAdvancingDistributedSystems2025,kathiresanCybersecurityRiskModeling2025}.

Bei den Methoden dominieren komplexe Modelle, besonders \gls{nn}.
Das liegt daran, dass sie mit unstrukturierten Betriebsdaten umgehen können, zum Beispiel mit Protokollen, Metriken und Ereignissen \cite{enemosahEnhancingDevOpsEfficiency2025}.
Daneben werden auch klassische Verfahren genutzt, etwa baumbasierte Modelle für Klassifikation und Prognosen sowie Clustering für die Gruppierung ähnlicher Fehlerbilder.
In mehreren Arbeiten wird deutlich, dass einfachere und besser nachvollziehbare Verfahren in manchen Fällen ausreichen würden, in der Forschung aber seltener im Fokus stehen \cite{govindarajanMachineLearningBased2025}.

Bei den Werkzeugen zeigt sich, dass viele Ansätze auf bestehenden Plattform- und Betriebswerkzeugen aufbauen und diese um \gls{ki}-Funktionen ergänzen.
Auf Basis historischer Betriebsdaten werden wiederkehrende Lastmuster und typische Fehlerbilder erkannt, um Skalierungs- und Diagnoseentscheidungen früher und gezielter zu treffen \cite{kankanalaAIMLDevOps2024,tamminediAutomatingKubernetesOperations2024}.
Im Kubernetes-Umfeld betrifft das zum Beispiel Autoscaling und die Auswertung von Events und Protokollen.
Im \gls{ci}/\gls{cd}-Kontext werden Werkzeuge wie K8sGPT \cite{zaaloukCLOUDNATIVEARTIFICIAL}, Jenkins X \cite{kankanalaAIMLDevOps2024} oder DeepCode \cite{supritpattanayakIntegratingAIDevOps2024} genannt, um Build- und Analyseaufgaben zu unterstützen.
Für die Rollout-Absicherung und den Betrieb kommen Plattformen wie Spinnaker oder Dynatrace \cite{supritpattanayakIntegratingAIDevOps2024} zum Einsatz, die Betriebsdaten auswerten und Auffälligkeiten während Auslieferung und Laufzeit identifizieren.
Im Sicherheitskontext werden Werkzeuge wie Trivy \cite{kankanalaAIMLDevOps2024}, Snyk \cite{sikhaCloudNativeApplicationDevelopment2023} oder Falco \cite{sikhaCloudNativeApplicationDevelopment2023} genannt, die Funde aus Scans und Laufzeitdaten priorisieren.


\subsection{Beantwortung der Forschungsfrage RQ3}
\label{rq3Antwort}
Eine qualitative Bewertung und Priorisierung von \gls{ki}-Use-Cases ist möglich, wenn \gls{ki}-Ansätze nicht isoliert betrachtet werden, sondern systematisch in den betrieblichen Kontext eingeordnet werden.
Als Ausgangspunkt dient die Einordnung nach Implementierungsaufwand und operativem Mehrwert \cite{tamminediAutomatingKubernetesOperations2024}.
Damit lässt sich jedoch nur grob abschätzen, ob ein Anwendungsfall grundsätzlich sinnvoll erscheint. Für eine belastbare Aussage zur praktischen Umsetzbarkeit und zur erwarteten Wirkung sind ergänzende Dimensionen erforderlich.

Die Umsetzbarkeit lässt sich anhand der verfügbaren Datenqualität und -historie, des technischen Fits zur bestehenden Plattformarchitektur sowie des erforderlichen Fachwissens für Betrieb und Wartung bewerten \cite{enemosahEnhancingDevOpsEfficiency2025,reddygopireddyIntegratingAIDevOps2022}.
Die betriebliche Wirksamkeit ergibt sich daraus, dass der Use Case im laufenden Betrieb stabil einen messbaren Nutzen entfaltet und mit vertretbarem Aufwand auf weitere Services übertragbar ist \cite{kankanalaAIMLDevOps2024,tamminediAutomatingKubernetesOperations2024}.
Ergänzend beeinflussen Anforderungen aus Datenschutz und Governance sowie der Reifegrad der jeweiligen Lösung, ob ein Einsatz realistisch und dauerhaft tragfähig ist \cite{supritpattanayakIntegratingAIDevOps2024,uddohAIBasedThreatDetection2021}.

Die qualitative Bewertung entlang einheitlicher Kriterien ermöglicht es, \gls{ki}-Use-Cases systematisch einzuordnen und miteinander zu vergleichen.
Dabei ist nicht entscheidend, welche konkrete \gls{ki}-Methode oder welches Werkzeug eingesetzt wird.
Maßgeblich ist vielmehr, in welchem Verhältnis Implementierungsaufwand, erwarteter Nutzen und betriebliche Rahmenbedingungen zueinander stehen.
Die Übertragbarkeit von \gls{ki}-Lösungen ergibt sich damit nicht aus der Technologie selbst, sondern aus ihrer Passung zur Datenbasis, zu bestehenden Betriebsprozessen und zur organisatorischen Struktur der Plattform.

Die Anwendung des Bewertungskonzepts auf die \gls{bmlp} zeigt, dass sich generische Use-Case-Muster aus der Literatur auf einen konkreten Plattformkontext übertragen lassen.
Use Cases mit vorhandener Datenbasis und klarer Anbindung an bestehende Prozesse lassen sich nachvollziehbar bewerten und priorisieren.
Gleichzeitig werden Anwendungsfälle sichtbar, deren Umsetzung zwar theoretisch möglich ist, bei denen jedoch Aufwand oder betriebliche Risiken aktuell überwiegen.
Damit wird eine fundierte Entscheidungsgrundlage geschaffen, die über eine rein werkzeug- oder technologiegetriebene Betrachtung hinausgeht.

RQ3 wird damit beantwortet, indem ein nachvollziehbarer Zusammenhang zwischen \gls{ki}-Ansätzen, konkreten Plattform-Use-Cases und deren Umsetzbarkeit hergestellt wird.