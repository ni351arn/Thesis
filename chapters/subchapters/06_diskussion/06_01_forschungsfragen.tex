\section{Beantwortung der Forschungsfragen}
\label{sec:beantwortung-ff}
In diesem Abschnitt werden die drei Forschungsfragen der Arbeit (RQ1, RQ2, RQ3) auf Basis der erarbeiteten Ergebnisse beantwortet. 
Die Antworten beziehen sich auf die in der Literaturanalyse identifizierten Anwendungsfelder und Herausforderungen (RQ1), die untersuchten KI-Methoden und -Werkzeuge (RQ2) sowie das entwickelte Bewertungskonzept und Erkenntnisse aus Fallstudien (RQ3). 
Dadurch werden die in Kapitel 3.2 formulierten Fragen detailliert adressiert und die Erkenntnisse in den Gesamtkontext des Cloud-Native Platform Engineerings eingeordnet.

\subsection{Beantwortung der Forschungsfrage RQ1}
\label{rq1Antwort}
Die Literaturanalyse zeigt vier wiederkehrende Anwendungsfelder, in denen KI einen Mehrwert bringt und gleichzeitig auch Herausforderungen mit sich bringt.
Diese lassen sich entlang konkreter betrieblicher Aufgaben abgrenzen: (1) Ressourcen- und Workload-Optimierung, (2) Betrieb und Orchestrierung der Plattform, (3) Optimierung von CI/CD-Pipelines sowie (4) Sicherheits- und Bedrohungserkennung. 

Das Proaktiven Ressourcen-Management thematisiert die Skalierung von Workloads, zum Beispiel um die Anzahl von Kubernetes-Pods bei schwankender Last.
Die KI wertet Auslastungs- und Latenzverläufe aus und leitet daraus eine Vorhersage für die nächste Lastphase ab.
Auf Basis dieser Vorhersage wird eine Skalierungsentscheidung früher getroffen als bei fester Zuteilung.
Dadurch sinkt das Risiko, dass Services zu spät skalieren und unter Last instabil werden.
Gleichzeitig wird eine Überbereitstellung reduziert, weil nicht zu viel Kapazität vorgehalten wird.
Typische Herausforderungen entstehen, wenn nicht ausreichend historische Daten vorliegen oder Lastmuster sich häufig ändern.

Betrieb und Orchestrierung adressiert Störungen im laufenden Betrieb und die Eingrenzung ihrer Ursachen über mehrere Komponenten hinweg.
Die Grundlage bilden Protokolle, Metriken und Ereignisse, die zu einen Hinweis geben, welche Komponente wahrscheinlich der Auslöser ist.
Ein typisches Beispiel ist die Auswertung von Kubernetes-Events und Protokollen, um Fehlkonfigurationen oder wiederkehrende Fehlerbilder schneller zu erkennen.
Der Mehrwert liegt in kürzerer Fehlersuche und einer schnelleren Einordnung, was wirklich relevant ist.
Der Ansatz wird unzuverlässig, wenn sich das Normalverhalten durch Releases und Konfigurationsänderungen ständig verschiebt und dadurch viele Fehlalarme entstehen.

Die Absicherung von Ausrollungen zielt darauf ab, fehlerhafte Versionen früh zu begrenzen.
Während einer schrittweisen Ausrollung werden Fehlerraten und Antwortzeiten beobachtet und mit dem erwarteten Normalzustand verglichen.
Bei auffälligen Abweichungen kann eine Ausrollung gestoppt oder auf die vorige Version zurückgesetzt werden.
Das ist relevant, weil Ausrollungen regelmäßig sind und Fehler sonst erst sichtbar werden, wenn sie bereits breiter wirken.
Schwierig wird es, wenn normale Schwankungen nicht sauber von echten Fehlern getrennt werden und dadurch unnötige Rücksetzungen ausgelöst werden.

Sicherheits- und Bedrohungserkennung umfasst die Anomalieerkennung, Echtzeiterkennung von Angriffen sowie die Klassifikation von Bedrohungen.
Ein greifbares Beispiel ist der Einsatz von SL-Algorithmen um erkannte Anomaliens als bösaritg oder gutartig einzustufen. 
Ergänzend können Laufzeitdaten Hinweise liefern, ob ein Fund in der konkreten Nutzung überhaupt wirksam wird.
Dadurch fließt weniger Zeit in unkritische Funde und relevante Punkte können schneller bearbeitet werden.
Grenzen entstehen durch sensible Daten und Vorgaben, weil Protokolle und Sicherheitsdaten nicht beliebig ausgewertet und aufbewahrt werden dürfen.

Über alle Felder hinweg zeigen sich wiederkehrende Herausforderungen.
Dazu zählen zusätzlicher Rechen- und Betriebsaufwand, Integrationsaufwand in bestehende Abläufe sowie Einschränkungen durch Datenschutz und Freigaben.
Am häufigsten limitiert jedoch die Datenbasis, weil unvollständige oder uneinheitliche Daten direkt zu instabilen Ergebnissen führen.

\subsection{Beantwortung der Forschungsfrage RQ2}
\label{rq2Antwort}
Die Literatur zeigt kein einheitliches Lernparadigma, Werkzeug oder Algorithmen, die für alle Anwendungsfälle gleichermaßen geeignet sind.
Die Wahl hängt vor allem vom Anwendungsfall, von der verfügbaren Datenbasis und von der Systemarchitektur der Plattform ab.

Am häufigsten werden Supervised Learning Verfahren thematisiert und eingesetzt.
Sie passen dort, wo historische Daten mit bekannten Ergebnissen vorliegen, zum Beispiel erfolgreiche und fehlgeschlagene Builds oder klassifizierte Störungen.
Damit lassen sich Vorhersagen für Lastverläufe erstellen oder Fehlertypen aus Protokollen einordnen.
Auch die Bewertung von Änderungsrisiken in CI/CD-Pipelines wird so umgesetzt, indem frühere Pipeline-Verläufe als Trainingsgrundlage dienen.

Unsupervised Learning Verfahren werden dementsprechend dort eingesetzt, wo solche Zuordnungen fehlen.
Typisch ist die Erkennung von Abweichungen in Metriken oder Protokollen, ohne dass vorher markiert wurde, was ein Fehler ist.
Das ist im Betrieb hilfreich, führt aber in dynamischen Umgebungen oft zu Fehlalarmen, weil sich das Normalverhalten durch Releases und Konfigurationsänderungen verschiebt.

Etwas weniger thematisiert aber trotzdem relevant sind Reinforcement Learning Ansätze.
Diese werden vor allem für Steuerungsentscheidungen beschrieben, zum Beispiel für Skalierung oder Rollout-Entscheidungen unter wechselnden Bedingungen.
Der Aufwand ist hoch, weil Training und Rückkopplung sauber abgebildet werden müssen, daher ist der Einsatz in produktiven Umgebungen bislang begrenzt. 

Bei den Methoden dominieren komplexe Modelle, besonders neuronale Netze.
Das liegt daran, dass sie mit heterogenen Betriebsdaten umgehen können, zum Beispiel mit Protokollen, Metriken und Ereignissen.
Daneben werden auch klassische Verfahren genutzt, etwa baumbasierte Modelle für Klassifikation und Prognosen, oder Clustering für die Gruppierung ähnlicher Fehlerbilder.
In mehreren Arbeiten wird deutlich, dass einfachere und besser nachvollziehbare Verfahren in manchen Fällen ausreichen würden, in der Forschung aber seltener im Fokus stehen.

Bei den Werkzeugen zeigt sich ein ähnliches Bild.
Viele Ansätze bauen auf bestehenden Plattform- und Betriebswerkzeugen auf und ergänzen diese um KI-Funktionen.
Im Kubernetes-Umfeld betrifft das zum Beispiel Autoscaling und die Auswertung von Events und Protokollen.
Im CI/CD-Kontext werden Werkzeuge wie K8sGPT, Jenkins X oder DeepCode genannt, um Build- und Analyseaufgaben zu unterstützen.
Für Rollout-Absicherung und Betrieb werden auch Plattformen wie Spinnaker oder Dynatrace beschrieben, die Daten aus dem Betrieb auswerten und Hinweise ableiten.
Im Sicherheitskontext werden Werkzeuge wie Trivy, Snyk oder Falco genannt, die Funde aus Scans und Laufzeitdaten priorisieren.

Insgesamt zeigen die Ergebnisse, dass Verfahren und Werkzeuge nur dann sinnvoll einzuordnen sind, wenn der konkrete Use Case und die zugrunde liegende Datenbasis mitbetrachtet werden.


\subsection{Beantwortung der Forschungsfrage RQ3}
\label{rq3Antwort}

Die Forschungsfrage RQ3 untersucht, wie sich identifizierte KI-Lösungen auf typische Anwendungsfälle im Platform Engineering übertragen und hinsichtlich ihres Mehrwerts und ihrer Umsetzbarkeit bewerten lassen. 
Die Ergebnisse dieser Arbeit zeigen, dass eine strukturierte Bewertung möglich ist, wenn technische Ansätze nicht isoliert betrachtet, sondern in einen betrieblichen Kontext eingeordnet werden.

Als Grundlage dient das in Kapitel 5.1 entwickelte Bewertungskonzept, das zwei zentrale Dimensionen kombiniert. 
Die erste Dimension beschreibt den Implementierungsaufwand und wird auf der X-Achse abgebildet. 
Sie reicht von niedrigem bis hohem Aufwand und orientiert sich daran, in welchem Umfang bestehende Funktionen genutzt werden können, ob Anpassungen erforderlich sind oder ob ein eigenständiger Aufbau und kontinuierlicher Betrieb notwendig sind. 
Ein niedriger Implementierungsaufwand liegt vor, wenn vorhandene Werkzeuge oder Funktionen ohne eigenes Modelltraining eingesetzt werden können. 
Ein hoher Aufwand ist gegeben, wenn umfangreiche Integration, eigene Modellanpassungen sowie laufende Überwachung und Wartung erforderlich sind.

Die zweite Dimension bildet den operativen Mehrwert ab und wird auf der Y-Achse dargestellt. 
Auch hier erfolgt die Einordnung von niedrig bis hoch. 
Maßgeblich ist, in welchem Umfang ein Use Case zur messbaren Verbesserung zentraler Betriebsziele beiträgt. 
Ein niedriger operativer Mehrwert liegt vor, wenn KI lediglich unterstützende Funktionen erfüllt. 
Ein hoher Mehrwert ist gegeben, wenn sich deutliche Verbesserungen in Bereichen wie Stabilität, Reaktionszeiten oder Ressourceneffizienz erwarten lassen.

Zur Ergänzung dieser zweidimensionalen Einordnung wurden in Kapitel 5.1 zusätzliche Bewertungsdimensionen eingeführt. 
Die Dimension Umsetzbarkeit betrachtet, ob die notwendigen Datenquellen verfügbar sind, wie gut sich der Use Case in bestehende Plattformarchitekturen integrieren lässt und welches fachliche Know-how erforderlich ist. 
Die Dimension Betriebswirksamkeit und Skalierbarkeit bewertet, ob der erwartete Nutzen im laufenden Betrieb stabil erzielt werden kann und ob sich der Ansatz über mehrere Dienste oder Teams hinweg übertragen lässt. 
Die Dimension Governance und Reifegrad berücksichtigt Anforderungen an Datenschutz, Sicherheit und Nachvollziehbarkeit sowie den Entwicklungsstand der jeweiligen Lösung.

Die Bewertung entlang aller Dimensionen erfolgt jeweils qualitativ in den Stufen niedrig, mittel und hoch. 
Diese Einordnung orientiert sich nicht an einzelnen Technologien, sondern an strukturellen Kriterien wie Datenverfügbarkeit, Integrationsaufwand und betrieblicher Wirkung. 
Dadurch wird eine vergleichbare Bewertung unterschiedlicher Anwendungsfälle ermöglicht, ohne konkrete Umsetzungen vorwegzunehmen.

Insgesamt zeigt sich, dass die Übertragbarkeit von KI-Lösungen im Platform Engineering weniger von einzelnen Methoden abhängt als von einer konsistenten Betrachtung von Aufwand, Nutzen und betrieblichen Rahmenbedingungen. 
Das Bewertungskonzept ermöglicht es, generische Use-Case-Muster aus der Literatur systematisch auf konkrete Plattformkontexte zu beziehen und fundierte Priorisierungsentscheidungen vorzubereiten. 
Damit beantwortet die Arbeit die Forschungsfrage RQ3 auf einer strukturellen Ebene und schafft eine Brücke zwischen den Ergebnissen der Literaturanalyse und ihrer praktischen Einordnung.